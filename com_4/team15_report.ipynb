{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Member\n",
    "\n",
    "### 107062514 賴鵬仁\n",
    "### 107062616 傅品捷\n",
    "### 107065513 姚定嘉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing的部分完全按照助教給的做法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import re\n",
    "\n",
    "import string\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 2428 -> polkadots\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path+'/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path+'/word2Id.npy'))\n",
    "id2word_dict =  dict(np.load(dictionary_path+'/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s'%('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s'%('2428', id2word_dict['2428']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s'%(word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    tokens = []\n",
    "    tokens.extend(nltk.tokenize.word_tokenize(prep_line.lower()))\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [word2Id_dict[tokens[k]] if tokens[k] in word2Id_dict else word2Id_dict['<RARE>'] for k in range(len(tokens))]\n",
    "    \n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path+'/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data'%(n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "import random\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "def load_image_array(image_file, image_size):\n",
    "    img = skimage.io.imread(image_file)\n",
    "    # GRAYSCALE\n",
    "    if len(img.shape) == 2:\n",
    "        img_new = np.ndarray( (img.shape[0], img.shape[1], 3), dtype = 'uint8')\n",
    "        img_new[:,:,0] = img\n",
    "        img_new[:,:,1] = img\n",
    "        img_new[:,:,2] = img\n",
    "        img = img_new\n",
    "\n",
    "    img_resized = skimage.transform.resize(img, (image_size, image_size))\n",
    "\n",
    "    # FLIP HORIZONTAL WIRH A PROBABILITY 0.5\n",
    "    if random.random() > 0.5:\n",
    "        img_resized = np.fliplr(img_resized)\n",
    "    \n",
    "    \n",
    "    return img_resized.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_wrong_img(self):\n",
    "    wrong_images = np.zeros((64, 64, 64, 3))\n",
    "    cnt = 0\n",
    "    for i in range(64):\n",
    "        wrong_image_id = random.randint(0,7370)\n",
    "        impath = df['ImagePath'].values[wrong_image_id]\n",
    "        wrong_image_array = load_image_array(data_path+impath,64)\n",
    "        wrong_images[cnt, :,:,:] = wrong_image_array\n",
    "        cnt+=1\n",
    "    return wrong_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n"
     ]
    }
   ],
   "source": [
    "#df.head(5)\n",
    "#print(df.ix[:3, \"Captions\"])\n",
    "#print((df['Captions'].values[4])[2])\n",
    "captions = df['Captions'].values\n",
    "caption = []\n",
    "for i in range(64):\n",
    "    caption.append(random.choice(captions[i])) \n",
    "caption = np.asarray(caption)\n",
    "print(caption.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_DEPTH = 3\n",
    "def training_data_generator(caption,image_path):\n",
    "    # load in the image according to image path\n",
    "    imagefile = tf.read_file(data_path +  image_path)\n",
    "    image = tf.image.decode_image(imagefile, channels=3)\n",
    "    float_img = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    float_img.set_shape([None, None, 3])\n",
    "    image = tf.image.resize_images(float_img, size = [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    image.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    #image = tf.image.random_brightness(image, max_delta=63)\n",
    "    return image, caption\n",
    "\n",
    "def data_iterator(filenames, batch_size, data_generator):\n",
    "    # Load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i])) \n",
    "    caption = np.asarray(caption)\n",
    "    image_path = df['ImagePath'].values\n",
    "\n",
    "    # Assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    \n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "BATCH_SIZE = 64\n",
    "iterator_train, types, shapes = data_iterator(data_path+'/text2ImgData.pkl', BATCH_SIZE, training_data_generator)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator_train.initializer)\n",
    "    next_element = iterator_train.get_next()\n",
    "    image, text = sess.run(next_element)\n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text (a list of id)\n",
    "    output: hidden representation of input text in dimention of TEXT_DIM\n",
    "    \"\"\"\n",
    "    def __init__(self, text, hparas, training_phase=True, reuse=False, return_embed=False):\n",
    "        self.text = text\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope('rnnftxt', reuse=self.reuse):\n",
    "            # Word embedding\n",
    "            word_embed_matrix = tf.get_variable('rnn/wordembed', \n",
    "                                                shape=(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM']),\n",
    "                                                initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                                                dtype=tf.float32)\n",
    "            embedded_word_ids = tf.nn.embedding_lookup(word_embed_matrix, self.text)\n",
    "            # RNN encoder\n",
    "            LSTMCell = tf.nn.rnn_cell.LSTMCell(self.hparas['TEXT_DIM'], reuse=self.reuse)\n",
    "            initial_state = LSTMCell.zero_state(self.hparas['BATCH_SIZE'], dtype=tf.float32)\n",
    "            rnn_net = tf.nn.dynamic_rnn(cell=LSTMCell, \n",
    "                                        inputs=embedded_word_ids, \n",
    "                                        initial_state=initial_state, \n",
    "                                        dtype=np.float32, time_major=False,\n",
    "                                        scope='rnn/dynamic')\n",
    "            self.rnn_net = rnn_net\n",
    "            self.outputs = rnn_net[0][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_, output_dim, \n",
    "           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
    "           name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
    "\n",
    "        return conv\n",
    "\n",
    "def deconv2d(input_, output_shape,\n",
    "             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n",
    "             name=\"deconv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        w = tf.get_variable('w', [k_h, k_h, output_shape[-1], input_.get_shape()[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "\n",
    "        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,strides=[1, d_h, d_w, 1])\n",
    "\n",
    "\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "        \n",
    "        return deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, noise_z, text, training_phase, hparas, reuse):\n",
    "        self.z = noise_z\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.gf_dim = 128\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        relu = tf.nn.relu\n",
    "        #deconv2d = tf.layers.conv2d_transpose\n",
    "        bn = tf.layers.batch_normalization\n",
    "        linear = tf.layers.dense\n",
    "        with tf.variable_scope('generator', reuse=self.reuse):\n",
    "            text_flatten = tf.contrib.layers.flatten(self.text)\n",
    "            text_input = tf.layers.dense(text_flatten, self.hparas['TEXT_DIM'], name='generator/text_input', reuse=self.reuse)\n",
    "            z_text_concat = tf.concat([self.z, text_input], axis=1, name='generator/z_text_concat')\n",
    "            #g_net = tf.layers.dense(z_text_concat, 64*64*3, name='generator/g_net', reuse=self.reuse)\n",
    "            \n",
    "            s = 64\n",
    "            s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16)\n",
    "            \n",
    "            \n",
    "            z_ = linear(z_text_concat, self.gf_dim*8*s16*s16)\n",
    "            h0 = tf.reshape(z_, [-1, s16, s16, self.gf_dim * 8])\n",
    "            h0 = relu(bn(h0, training=True))\n",
    "            \n",
    "            h1 = deconv2d(h0, [self.hparas['BATCH_SIZE'], s8, s8, self.gf_dim*4], name='g_h1')\n",
    "            h1 = relu(bn(h1, training=True))\n",
    "            \n",
    "            h2 = deconv2d(h1, [self.hparas['BATCH_SIZE'], s4, s4, self.gf_dim*2], name='g_h2')\n",
    "            h2 = relu(bn(h2, training=True))\n",
    "            \n",
    "            h3 = deconv2d(h2, [self.hparas['BATCH_SIZE'], s2, s2, self.gf_dim*1], name='g_h3')\n",
    "            h3 = relu(bn(h3, training=True))\n",
    "            \n",
    "            h4 = deconv2d(h3, [self.hparas['BATCH_SIZE'], s, s, 3], name='g_h4')\n",
    "            \n",
    "            #g_net = tf.reshape(g_net, [-1, 64, 64, 3], name='generator/g_net_reshape')\n",
    "            \n",
    "            g_net = tf.nn.tanh(h4)\n",
    "            \n",
    "            self.generator_net = g_net\n",
    "            self.outputs = g_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input, name):\n",
    "    input_shape = input.get_shape()\n",
    "    dim = 1\n",
    "    for d in input_shape[1:].as_list():\n",
    "        dim *= d\n",
    "        input = tf.reshape(input, [-1, dim])\n",
    "    \n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pretrain:\n",
    "    def __init__(self, intput, hparas,  training_phase=True, reuse=False):\n",
    "        self.input=intput\n",
    "        self.hparas=hparas\n",
    "        self.train = training_phase\n",
    "        self.reuse = reuse\n",
    "        self.df_dim = 128 \n",
    "        self._build_model()\n",
    "    def _build_model(self):\n",
    "        lrelu = tf.nn.leaky_relu\n",
    "        bn = tf.layers.batch_normalization\n",
    "        with tf.variable_scope('cnnftxt', reuse=self.reuse):\n",
    "            h0=lrelu(bn(conv2d(self.input, self.df_dim,4,4,2,2, name = 'pre_h0' ), training=True))\n",
    "            h1=lrelu(bn(conv2d(h0,self.df_dim*2,4,4,2,2,name = 'pre_h1' ), training=True))\n",
    "            h2=lrelu(bn(conv2d(h1,self.df_dim*4,4,4,2,2,name = 'pre_h2' ), training=True))\n",
    "            h3=lrelu(bn(conv2d(h2,self.df_dim*8,4,4,2,2,name = 'pre_h3' ), training=True))\n",
    "            h4 = flatten(h3,name = 'pre_h4')\n",
    "            h4 = tf.layers.dense(h4, units = self.hparas['TEXT_DIM'])\n",
    "            self.outputs = h4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet structure\n",
    "class Discriminator:\n",
    "    def __init__(self, image, text, training_phase, hparas, reuse):\n",
    "        self.image = image\n",
    "        self.text = text\n",
    "        self.train = training_phase\n",
    "        self.hparas = hparas\n",
    "        self.df_dim = 128\n",
    "        self.reuse = reuse\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self): \n",
    "        lrelu = tf.nn.leaky_relu\n",
    "        #conv2d = tf.layers.conv2d\n",
    "        bn = tf.layers.batch_normalization\n",
    "        linear = tf.layers.dense\n",
    "        with tf.variable_scope('discriminator', reuse=self.reuse):\n",
    "            \n",
    "            h0 = lrelu(conv2d(self.image, self.df_dim, name = 'd_h0_conv')) \n",
    "            h1 = lrelu(bn(conv2d(h0, self.df_dim*2, name = 'd_h1_conv'), training=True)) \n",
    "            h2 = lrelu(bn(conv2d(h1, self.df_dim*4, name = 'd_h2_conv'), training=True)) \n",
    "            h3 = lrelu(bn(conv2d(h2, self.df_dim*8, name = 'd_h3_conv'), training=True)) \n",
    "\n",
    "            reduced_text_embeddings = lrelu(linear(self.text,self.df_dim))\n",
    "            reduced_text_embeddings = tf.expand_dims(reduced_text_embeddings,1)\n",
    "            reduced_text_embeddings = tf.expand_dims(reduced_text_embeddings,2)\n",
    "            tiled_embeddings = tf.tile(reduced_text_embeddings, [1,4,4,1], name='tiled_embeddings')\n",
    "\n",
    "            h3_concat = tf.concat( [h3, tiled_embeddings], axis = 3,name='h3_concat')\n",
    "            h3_new = lrelu(bn(conv2d(h3_concat, self.df_dim*8, 1,1,1,1, name = 'd_h3_conv_new')))\n",
    "            d_net = linear(tf.reshape(h3_new, [self.hparas['BATCH_SIZE'], -1]), 1,)\n",
    "            self.logits = d_net\n",
    "            self.discriminator_net = d_net\n",
    "            self.outputs =  tf.nn.sigmoid(d_net)          \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://imgur.com/a/JFOToWV\n",
    "\n",
    "我們遵循trick第7點提到的：\"Use DCGAN when you can. It works!\"，將助教給的generator和discriminator改成DCGAN所使用的架構，也就是將fully connected換成convolution layer和deconvolution layer，實驗結果證明效果好非常多。\n",
    "\n",
    "Generator：先將text representation和image concat起來，當作generator的input。\n",
    "\n",
    "Discriminator：將image經過convolution layer之後，和text embedding結合起來，產生最後的結果。\n",
    "\n",
    "基本的想法如同上圖所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparas():\n",
    "    hparas = {\n",
    "        'MAX_SEQ_LENGTH' : 20,\n",
    "        'EMBED_DIM' : 256, # word embedding dimension\n",
    "        'VOCAB_SIZE' : len(vocab),\n",
    "        'TEXT_DIM' : 128, # text embedding dimension\n",
    "        'RNN_HIDDEN_SIZE' : 128,\n",
    "        'Z_DIM' : 512, # random noise z dimension\n",
    "        'IMAGE_SIZE' : [64, 64, 3], # render image size\n",
    "        'BATCH_SIZE' : 64,\n",
    "        'LR' : 0.0002,\n",
    "        'BETA' : 0.5, # AdamOptimizer parameter\n",
    "        'N_EPOCH' : 100,\n",
    "        'N_SAMPLE' : num_training_sample\n",
    "    }\n",
    "    return hparas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參數tuning\n",
    "\n",
    "在此我們改了 TEXT_DIM，RNN_HIDDEN_SIZE，Z_DIM，EMBED_DIM，一開始助教給的dimension皆為16，我們認為這麼少的維度，沒有辦法充份表現出該有的representation，所以改成如上的參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    cost = tf.reduce_sum(tf.multiply(v1, v2), 1) / (tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1)) * tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, hparas, training_phase, dataset_path, ckpt_path, inference_path, recover=None):\n",
    "        self.hparas = hparas\n",
    "        self.train = training_phase\n",
    "        self.dataset_path = dataset_path # dataPath+'/text2ImgData.pkl'\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.sample_path = './samples'\n",
    "        self.inference_path = './inference'\n",
    "        \n",
    "        self._get_session() # get session\n",
    "        self._get_train_data_iter() # initialize and get data iterator\n",
    "        self._input_layer() # define input placeholder\n",
    "        self._get_inference() # build generator and discriminator\n",
    "        self._get_loss() # define gan loss\n",
    "        self._get_var_with_name() # get variables for each part of model\n",
    "        self._optimize() # define optimizer\n",
    "        self._init_vars()\n",
    "        self._get_saver()\n",
    "        \n",
    "        if recover is not None:\n",
    "            self._load_checkpoint(recover)\n",
    "            \n",
    "            \n",
    "        \n",
    "    def _get_train_data_iter(self):\n",
    "        if self.train: # training data iteratot\n",
    "            iterator_train, types, shapes = data_iterator(self.dataset_path+'/text2ImgData.pkl',\n",
    "                                                          self.hparas['BATCH_SIZE'], training_data_generator)\n",
    "            iter_initializer = iterator_train.initializer\n",
    "            self.next_element = iterator_train.get_next()\n",
    "            self.sess.run(iterator_train.initializer)\n",
    "            self.iterator_train = iterator_train\n",
    "        else: # testing data iterator\n",
    "            iterator_test, types, shapes = data_iterator_test(self.dataset_path+'/testData.pkl', self.hparas['BATCH_SIZE'])\n",
    "            iter_initializer = iterator_test.initializer\n",
    "            self.next_element = iterator_test.get_next()\n",
    "            self.sess.run(iterator_test.initializer)\n",
    "            self.iterator_test = iterator_test\n",
    "            \n",
    "    def _input_layer(self):\n",
    "        if self.train:\n",
    "            self.real_image = tf.placeholder('float32',\n",
    "                                              [self.hparas['BATCH_SIZE'], self.hparas['IMAGE_SIZE'][0],\n",
    "                                               self.hparas['IMAGE_SIZE'][1], self.hparas['IMAGE_SIZE'][2]],\n",
    "                                              name='real_image')\n",
    "            '''add wrong image'''\n",
    "            self.wrong_image = tf.placeholder('float32',\n",
    "                                              [self.hparas['BATCH_SIZE'], self.hparas['IMAGE_SIZE'][0],\n",
    "                                               self.hparas['IMAGE_SIZE'][1], self.hparas['IMAGE_SIZE'][2]],\n",
    "                                              name='wrong_image')\n",
    "            '''add wrong image'''\n",
    "            self.caption = tf.placeholder(dtype=tf.int64, shape=[self.hparas['BATCH_SIZE'], None], name='caption')\n",
    "            self.wrong_caption = tf.placeholder(dtype=tf.int64, shape=[self.hparas['BATCH_SIZE'], None], name='wrong_caption')\n",
    "            self.z_noise = tf.placeholder(tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']], name='z_noise')\n",
    "            angles = tf.random_uniform([64], minval=-15.0 * np.pi / 180.0, maxval=15.0 * np.pi / 180.0, dtype=tf.float32, seed=9487)\n",
    "            self.real_image = tf.contrib.image.rotate(self.real_image, angles, interpolation='NEAREST')\n",
    "            self.wrong_image = tf.contrib.image.rotate(self.wrong_image, angles, interpolation='NEAREST')\n",
    "        else:\n",
    "            self.caption = tf.placeholder(dtype=tf.int64, shape=[self.hparas['BATCH_SIZE'], None], name='caption')\n",
    "            self.z_noise = tf.placeholder(tf.float32, [self.hparas['BATCH_SIZE'], self.hparas['Z_DIM']], name='z_noise')\n",
    "    \n",
    "    def _get_inference(self):\n",
    "        if self.train:\n",
    "            ### Training Phase - CNN - RNN mapping\n",
    "            net_cnn = pretrain(self.real_image, hparas = self.hparas, training_phase=True, reuse=False)\n",
    "            x = net_cnn.outputs\n",
    "            v = TextEncoder(self.caption, hparas = self.hparas, training_phase=True, reuse=False).outputs\n",
    "            x_w = pretrain(self.wrong_image, hparas = self.hparas, training_phase=True, reuse=True).outputs\n",
    "            v_w = TextEncoder(self.wrong_caption, hparas = self.hparas, training_phase=True, reuse=True).outputs\n",
    "            print(self.real_image.shape)\n",
    "            print(x.shape,v.shape)\n",
    "            alpha = 0.2 # margin alpha\n",
    "            rnn_loss = tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x, v_w))) + \\\n",
    "                        tf.reduce_mean(tf.maximum(0., alpha - cosine_similarity(x, v) + cosine_similarity(x_w, v)))\n",
    "            self.rnn_loss=rnn_loss\n",
    "            # GAN training\n",
    "            # encoding text\n",
    "            text_encoder = TextEncoder(self.caption, hparas = self.hparas, training_phase=True, reuse=True)\n",
    "            self.text_encoder = text_encoder\n",
    "            # generating image\n",
    "            generator = Generator(self.z_noise, text_encoder.outputs, training_phase=True,\n",
    "                                  hparas=self.hparas, reuse=False)\n",
    "            self.generator = generator\n",
    "            \n",
    "            # discriminize\n",
    "            # fake image\n",
    "            fake_discriminator = Discriminator(generator.outputs, text_encoder.outputs,\n",
    "                                               training_phase=True, hparas=self.hparas, reuse=False)\n",
    "            self.fake_discriminator = fake_discriminator\n",
    "            # real image\n",
    "            real_discriminator = Discriminator(self.real_image, text_encoder.outputs, training_phase=True,\n",
    "                                              hparas=self.hparas, reuse=True)\n",
    "            self.real_discriminator = real_discriminator\n",
    "            '''condition and real image not match'''\n",
    "            #third_con_discriminator = Discriminator(self.wrong_image, text_encoder.outputs, training_phase=True,\n",
    "            #                                  hparas=self.hparas, reuse=True)\n",
    "            \n",
    "            #self.third_con_discriminator = third_con_discriminator\n",
    "            \n",
    "            fourth_con_discriminator =  Discriminator(self.real_image, v_w, training_phase=True,\n",
    "                                              hparas=self.hparas, reuse=True)\n",
    "            self.fourth_con_discriminator = fourth_con_discriminator\n",
    "            '''condition and real image not match'''\n",
    "        else: # inference mode\n",
    "            \n",
    "            self.text_embed = TextEncoder(self.caption, hparas=self.hparas, training_phase=False, reuse=False)\n",
    "            self.generate_image_net = Generator(self.z_noise, self.text_embed.outputs, training_phase=False,\n",
    "                                                hparas=self.hparas, reuse=False)\n",
    "    def _get_loss(self):\n",
    "        if self.train:\n",
    "            d_loss1 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.real_discriminator.logits,\n",
    "                                                                              labels=tf.ones_like(self.real_discriminator.logits),\n",
    "                                                                              name='d_loss1'))\n",
    "            d_loss2 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_discriminator.logits,\n",
    "                                                                              labels=tf.zeros_like(self.fake_discriminator.logits),\n",
    "                                                                              name='d_loss2'))\n",
    "            '''add condition and real image not match loss'''\n",
    "            #d_loss3 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.third_con_discriminator.logits,\n",
    "            #                                                                  labels=tf.zeros_like(self.third_con_discriminator.logits),\n",
    "            #                                                                  name='d_loss3'))\n",
    "            d_loss4 =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fourth_con_discriminator.logits,\n",
    "                                                                              labels=tf.zeros_like(self.fourth_con_discriminator.logits),\n",
    "                                                                              name='d_loss4'))\n",
    "            '''add condition and real image not match loss'''\n",
    "            self.d_loss = d_loss1 + (d_loss2 + d_loss4)/2\n",
    "            self.g_loss =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.fake_discriminator.logits,\n",
    "                                                                                  labels=tf.ones_like(self.fake_discriminator.logits),\n",
    "                                                                                  name='g_loss'))\n",
    "            '''\n",
    "            alpha = tf.random_uniform(shape=self.real_image.get_shape(), minval=0.,maxval=1.)\n",
    "            differences = self.generator.outputs - self.real_image \n",
    "            interpolates = self.real_image + (alpha * differences)\n",
    "            D_inter = Discriminator(interpolates, self.text_encoder.outputs, training_phase=True,\n",
    "                                              hparas=self.hparas, reuse=True)\n",
    "            gradients = tf.gradients(D_inter.logits, [interpolates])[0]\n",
    "            slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "            gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "            self.d_loss += 0.25 * gradient_penalty\n",
    "            '''\n",
    "    \n",
    "    def _optimize(self):\n",
    "        if self.train:\n",
    "            with tf.variable_scope('learning_rate'):\n",
    "                self.lr_var = tf.Variable(self.hparas['LR'], trainable=False)\n",
    "\n",
    "            discriminator_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            generator_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            \n",
    "            self.d_optim = discriminator_optimizer.minimize(self.d_loss, var_list=self.discrim_vars)\n",
    "            self.g_optim = generator_optimizer.minimize(self.g_loss, var_list=self.generator_vars+self.text_encoder_vars)\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.rnn_loss, self.text_encoder_vars + self.cnn_vars), 10)\n",
    "            rnn_optimizer = tf.train.AdamOptimizer(self.lr_var, beta1=self.hparas['BETA'])\n",
    "            self.rnn_optim = rnn_optimizer.apply_gradients(zip(grads, self.text_encoder_vars + self.cnn_vars))\n",
    "\n",
    "        \n",
    "    def training(self):\n",
    "        \n",
    "        for _epoch in range(self.hparas['N_EPOCH']):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            n_batch_epoch = int(self.hparas['N_SAMPLE']/self.hparas['BATCH_SIZE'])\n",
    "            for _step in range(n_batch_epoch):\n",
    "                step_time = time.time()\n",
    "                image_batch, caption_batch = self.sess.run(self.next_element)\n",
    "                \n",
    "                '''add wrong img batch '''\n",
    "                wrong_images = np.zeros((64, 64, 64, 3))\n",
    "                cnt = 0\n",
    "                for i in range(64):\n",
    "                    wrong_image_id = random.randint(0,7370-1)\n",
    "                    impath = df['ImagePath'].values[wrong_image_id]\n",
    "                    wrong_image_array = load_image_array(data_path+impath,64)\n",
    "                    wrong_images[cnt, :,:,:] = wrong_image_array\n",
    "                    cnt+=1\n",
    "                wrong_batch = wrong_images\n",
    "                '''add wrong img batch '''\n",
    "                '''add wrong caption batch'''\n",
    "                #print(caption_batch.shape)\n",
    "                captions = df['Captions'].values\n",
    "                caption = []\n",
    "                for _ in range(64):\n",
    "                    wrong_caption_id = random.randint(0,7370-1)\n",
    "                    caption.append(random.choice(captions[wrong_caption_id])) \n",
    "                caption = np.asarray(caption)\n",
    "\n",
    "                wrong_caption_batch=caption\n",
    "                #wrong_caption_batch=caption_batch\n",
    "                '''add wrong caption batch'''\n",
    "                \n",
    "                b_z = np.random.normal(0, np.exp(-1 / np.pi),\n",
    "                                       size=(self.hparas['BATCH_SIZE'], self.hparas['Z_DIM'])).astype(np.float32)\n",
    "                \n",
    "                \n",
    "                if _epoch < 90:\n",
    "                    self.errRNN, _ = self.sess.run([self.rnn_loss, self.rnn_optim], feed_dict={\n",
    "                                                self.real_image:image_batch,\n",
    "                                                self.wrong_image:wrong_batch,\n",
    "                                                self.caption:caption_batch,\n",
    "                                                self.wrong_caption:wrong_caption_batch})\n",
    "                else:\n",
    "                    self.errRNN = 0\n",
    "\n",
    "\n",
    "                # update discriminator\n",
    "                for _ in range(2):\n",
    "                    self.discriminator_error, _ = self.sess.run([self.d_loss, self.d_optim],\n",
    "                                                               feed_dict={\n",
    "                                                                    self.real_image:image_batch,\n",
    "                                                                    self.wrong_image:wrong_batch,###add###\n",
    "                                                                    self.wrong_caption:wrong_caption_batch,\n",
    "                                                                    self.caption:caption_batch,\n",
    "                                                                    self.z_noise:b_z})\n",
    "\n",
    "                # update generate\n",
    "                self.generator_error, _ = self.sess.run([self.g_loss, self.g_optim],\n",
    "                                                       feed_dict={self.caption: caption_batch, self.z_noise : b_z})\n",
    "                #update RNN\n",
    "               \n",
    "                \n",
    "                if _step%50==0:\n",
    "                    print(\"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4fs, d_loss: %.3f, g_loss: %.3f, rnn_loss: %.3f\" \\\n",
    "                            % (_epoch, self.hparas['N_EPOCH'], _step, n_batch_epoch, time.time() - step_time,\n",
    "                               self.discriminator_error, self.generator_error,self.errRNN))\n",
    "            if _epoch != 0 and (_epoch+1)%5==0:\n",
    "                self._save_checkpoint(_epoch)\n",
    "                self._sample_visiualize(_epoch)\n",
    "            \n",
    "    def inference(self):\n",
    "        for _iters in range(100):\n",
    "            caption, idx = self.sess.run(self.next_element)\n",
    "            z_seed = np.random.normal(0, np.exp(-1 / np.pi), size=(self.hparas['BATCH_SIZE'], self.hparas['Z_DIM'])).astype(np.float32)\n",
    "\n",
    "            img_gen, rnn_out = self.sess.run([self.generate_image_net.outputs, self.text_embed.outputs],\n",
    "                                             feed_dict={self.caption : caption, self.z_noise : z_seed})\n",
    "            for i in range(self.hparas['BATCH_SIZE']):\n",
    "                scipy.misc.imsave(self.inference_path+'/inference_{:04d}.png'.format(idx[i]), img_gen[i])\n",
    "                \n",
    "    def _init_vars(self):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _get_session(self):\n",
    "        self.sess = tf.Session()\n",
    "    \n",
    "    def _get_saver(self):\n",
    "        if self.train:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            self.d_saver = tf.train.Saver(var_list=self.discrim_vars)\n",
    "        else:\n",
    "            self.rnn_saver = tf.train.Saver(var_list=self.text_encoder_vars)\n",
    "            self.g_saver = tf.train.Saver(var_list=self.generator_vars)\n",
    "            \n",
    "    def _sample_visiualize(self, epoch):\n",
    "        ni = int(np.ceil(np.sqrt(self.hparas['BATCH_SIZE'])))\n",
    "        sample_size = self.hparas['BATCH_SIZE']\n",
    "        max_len = self.hparas['MAX_SEQ_LENGTH']\n",
    "        \n",
    "        sample_seed = np.random.normal(0, np.exp(-1 / np.pi), size=(sample_size, self.hparas['Z_DIM'])).astype(np.float32)\n",
    "        sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"]*int(sample_size/ni) + [\"this flower has petals that are yellow, white and purple and has dark lines\"]*int(sample_size/ni) + [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) + [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "        for i, sent in enumerate(sample_sentence):\n",
    "            sample_sentence[i] = sent2IdList(sent, max_len)\n",
    "            \n",
    "        img_gen, rnn_out = self.sess.run([self.generator.outputs, self.text_encoder.outputs],\n",
    "                                         feed_dict={self.caption : sample_sentence, self.z_noise : sample_seed})\n",
    "        save_images(img_gen, [ni, ni], self.sample_path+'/train_{:02d}.png'.format(epoch))\n",
    "        \n",
    "    def _get_var_with_name(self):\n",
    "        t_vars = tf.trainable_variables()\n",
    "\n",
    "        self.text_encoder_vars = [var for var in t_vars if 'rnn' in var.name]\n",
    "        self.generator_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        self.discrim_vars = [var for var in t_vars if 'discrim' in var.name]\n",
    "        self.cnn_vars = [var for var in tf.trainable_variables() if 'cnn' in var.name]\n",
    "    \n",
    "    def _load_checkpoint(self, recover):\n",
    "        if self.train:\n",
    "            self.rnn_saver.restore(self.sess, self.ckpt_path+'rnn_model_'+str(recover)+'.ckpt')\n",
    "            self.g_saver.restore(self.sess, self.ckpt_path+'g_model_'+str(recover)+'.ckpt')\n",
    "            self.d_saver.restore(self.sess, self.ckpt_path+'d_model_'+str(recover)+'.ckpt')\n",
    "        else:\n",
    "            self.rnn_saver.restore(self.sess, self.ckpt_path+'rnn_model_'+str(recover)+'.ckpt')\n",
    "            self.g_saver.restore(self.sess, self.ckpt_path+'g_model_'+str(recover)+'.ckpt')\n",
    "        print('-----success restored checkpoint--------')\n",
    "    \n",
    "    def _save_checkpoint(self, epoch):\n",
    "        self.rnn_saver.save(self.sess, self.ckpt_path+'rnn_model_'+str(epoch)+'.ckpt')\n",
    "        self.g_saver.save(self.sess, self.ckpt_path+'g_model_'+str(epoch)+'.ckpt')\n",
    "        self.d_saver.save(self.sess, self.ckpt_path+'d_model_'+str(epoch)+'.ckpt')\n",
    "        print('-----success saved checkpoint--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)我們將原本助教給的loss再加上一項。有3種情況，首先是real image + true caption，這是real的情況；其餘2種為fake的情況，分別是real image + wrong caption 和 fake image + true caption。我們多加上了real image + wrong caption的loss，如果沒有考慮這種情況，那麼，Generator會無視condition，只產生跟real image很相像的圖即可成功騙過Discriminator，這明顯不是我們想要的效果。(單用此與data augmentation ， kaggle 分數最高到0.1319)\n",
    "\n",
    "(2)我們一開始跑了一些發現圖片和Caption並沒有很相關，因此認為RNN那邊可以改善，之後我們加入了pretrain改善了不少。我們將real image、true caption、fake image、wrong caption去做cosine_similarity當成RNN的loss，期望使圖片和句子更有關聯。 (加上pretrain 後 ， kaggle 分數到達0.127)\n",
    "\n",
    "(3)Data augmentation 我們採用了flip跟rotate。\n",
    "\n",
    "(4)在此有試過WGAN ，improved WGAN 其實效果都沒有很顯著的改善。\n",
    "\n",
    "(5)有試過loss再加上wrong image + real caption 情況，效果也沒比較好。\n",
    "\n",
    "(6)引用 training trick 第三點 \"Use a spherical Z\"，diversity看起來有好一點。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 64, 3)\n",
      "(64, 128) (64, 128)\n",
      "Epoch: [ 0/100] [   0/ 115] time: 7.9404s, d_loss: 1.700, g_loss: 3.392, rnn_loss: 0.434\n",
      "Epoch: [ 0/100] [  50/ 115] time: 2.0000s, d_loss: 1.014, g_loss: 3.555, rnn_loss: 0.361\n",
      "Epoch: [ 0/100] [ 100/ 115] time: 2.0888s, d_loss: 0.995, g_loss: 3.606, rnn_loss: 0.241\n",
      "Epoch: [ 1/100] [   0/ 115] time: 2.0193s, d_loss: 1.063, g_loss: 3.984, rnn_loss: 0.279\n",
      "Epoch: [ 1/100] [  50/ 115] time: 2.0183s, d_loss: 0.976, g_loss: 3.744, rnn_loss: 0.283\n",
      "Epoch: [ 1/100] [ 100/ 115] time: 2.0000s, d_loss: 1.035, g_loss: 3.765, rnn_loss: 0.202\n",
      "Epoch: [ 2/100] [   0/ 115] time: 2.0625s, d_loss: 1.125, g_loss: 4.242, rnn_loss: 0.199\n",
      "Epoch: [ 2/100] [  50/ 115] time: 2.0000s, d_loss: 1.009, g_loss: 3.475, rnn_loss: 0.268\n",
      "Epoch: [ 2/100] [ 100/ 115] time: 2.0148s, d_loss: 1.002, g_loss: 3.236, rnn_loss: 0.199\n",
      "Epoch: [ 3/100] [   0/ 115] time: 2.0526s, d_loss: 1.007, g_loss: 3.820, rnn_loss: 0.202\n",
      "Epoch: [ 3/100] [  50/ 115] time: 2.0156s, d_loss: 1.014, g_loss: 3.387, rnn_loss: 0.293\n",
      "Epoch: [ 3/100] [ 100/ 115] time: 2.0106s, d_loss: 1.068, g_loss: 4.226, rnn_loss: 0.277\n",
      "Epoch: [ 4/100] [   0/ 115] time: 2.0469s, d_loss: 1.045, g_loss: 3.814, rnn_loss: 0.196\n",
      "Epoch: [ 4/100] [  50/ 115] time: 1.9668s, d_loss: 1.009, g_loss: 3.541, rnn_loss: 0.292\n",
      "Epoch: [ 4/100] [ 100/ 115] time: 1.9912s, d_loss: 1.009, g_loss: 3.508, rnn_loss: 0.200\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [ 5/100] [   0/ 115] time: 1.9986s, d_loss: 1.131, g_loss: 4.076, rnn_loss: 0.175\n",
      "Epoch: [ 5/100] [  50/ 115] time: 2.0046s, d_loss: 1.012, g_loss: 4.498, rnn_loss: 0.207\n",
      "Epoch: [ 5/100] [ 100/ 115] time: 1.9844s, d_loss: 0.743, g_loss: 3.692, rnn_loss: 0.079\n",
      "Epoch: [ 6/100] [   0/ 115] time: 2.0312s, d_loss: 0.889, g_loss: 4.646, rnn_loss: 0.122\n",
      "Epoch: [ 6/100] [  50/ 115] time: 2.0000s, d_loss: 0.842, g_loss: 3.956, rnn_loss: 0.201\n",
      "Epoch: [ 6/100] [ 100/ 115] time: 2.0321s, d_loss: 0.736, g_loss: 3.712, rnn_loss: 0.095\n",
      "Epoch: [ 7/100] [   0/ 115] time: 2.0625s, d_loss: 0.859, g_loss: 3.673, rnn_loss: 0.129\n",
      "Epoch: [ 7/100] [  50/ 115] time: 1.9891s, d_loss: 1.142, g_loss: 2.545, rnn_loss: 0.205\n",
      "Epoch: [ 7/100] [ 100/ 115] time: 2.0312s, d_loss: 0.554, g_loss: 4.900, rnn_loss: 0.092\n",
      "Epoch: [ 8/100] [   0/ 115] time: 1.9687s, d_loss: 0.848, g_loss: 4.518, rnn_loss: 0.130\n",
      "Epoch: [ 8/100] [  50/ 115] time: 2.0312s, d_loss: 0.818, g_loss: 3.015, rnn_loss: 0.193\n",
      "Epoch: [ 8/100] [ 100/ 115] time: 2.0469s, d_loss: 0.957, g_loss: 4.252, rnn_loss: 0.104\n",
      "Epoch: [ 9/100] [   0/ 115] time: 2.0625s, d_loss: 0.847, g_loss: 3.430, rnn_loss: 0.115\n",
      "Epoch: [ 9/100] [  50/ 115] time: 1.9559s, d_loss: 0.832, g_loss: 3.042, rnn_loss: 0.135\n",
      "Epoch: [ 9/100] [ 100/ 115] time: 2.0445s, d_loss: 0.598, g_loss: 4.794, rnn_loss: 0.092\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [10/100] [   0/ 115] time: 2.0849s, d_loss: 0.894, g_loss: 3.985, rnn_loss: 0.134\n",
      "Epoch: [10/100] [  50/ 115] time: 2.0428s, d_loss: 0.792, g_loss: 4.525, rnn_loss: 0.130\n",
      "Epoch: [10/100] [ 100/ 115] time: 2.0327s, d_loss: 0.591, g_loss: 4.765, rnn_loss: 0.091\n",
      "Epoch: [11/100] [   0/ 115] time: 2.0156s, d_loss: 0.890, g_loss: 4.295, rnn_loss: 0.153\n",
      "Epoch: [11/100] [  50/ 115] time: 2.0496s, d_loss: 0.704, g_loss: 3.762, rnn_loss: 0.096\n",
      "Epoch: [11/100] [ 100/ 115] time: 2.0677s, d_loss: 0.621, g_loss: 4.719, rnn_loss: 0.095\n",
      "Epoch: [12/100] [   0/ 115] time: 1.9683s, d_loss: 0.851, g_loss: 3.422, rnn_loss: 0.114\n",
      "Epoch: [12/100] [  50/ 115] time: 2.0156s, d_loss: 0.905, g_loss: 3.900, rnn_loss: 0.147\n",
      "Epoch: [12/100] [ 100/ 115] time: 1.9844s, d_loss: 0.705, g_loss: 4.328, rnn_loss: 0.096\n",
      "Epoch: [13/100] [   0/ 115] time: 2.0469s, d_loss: 0.891, g_loss: 4.846, rnn_loss: 0.153\n",
      "Epoch: [13/100] [  50/ 115] time: 1.9844s, d_loss: 0.882, g_loss: 3.205, rnn_loss: 0.154\n",
      "Epoch: [13/100] [ 100/ 115] time: 2.0156s, d_loss: 0.693, g_loss: 4.513, rnn_loss: 0.098\n",
      "Epoch: [14/100] [   0/ 115] time: 2.0469s, d_loss: 0.869, g_loss: 4.064, rnn_loss: 0.124\n",
      "Epoch: [14/100] [  50/ 115] time: 2.0781s, d_loss: 0.951, g_loss: 3.408, rnn_loss: 0.163\n",
      "Epoch: [14/100] [ 100/ 115] time: 2.0113s, d_loss: 0.743, g_loss: 3.139, rnn_loss: 0.097\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [15/100] [   0/ 115] time: 2.0584s, d_loss: 0.795, g_loss: 4.447, rnn_loss: 0.127\n",
      "Epoch: [15/100] [  50/ 115] time: 2.0172s, d_loss: 0.920, g_loss: 3.431, rnn_loss: 0.131\n",
      "Epoch: [15/100] [ 100/ 115] time: 1.9850s, d_loss: 0.797, g_loss: 4.206, rnn_loss: 0.129\n",
      "Epoch: [16/100] [   0/ 115] time: 1.9966s, d_loss: 0.859, g_loss: 4.057, rnn_loss: 0.137\n",
      "Epoch: [16/100] [  50/ 115] time: 2.0797s, d_loss: 0.867, g_loss: 2.955, rnn_loss: 0.169\n",
      "Epoch: [16/100] [ 100/ 115] time: 1.9956s, d_loss: 0.803, g_loss: 3.478, rnn_loss: 0.167\n",
      "Epoch: [17/100] [   0/ 115] time: 2.0209s, d_loss: 0.861, g_loss: 4.034, rnn_loss: 0.137\n",
      "Epoch: [17/100] [  50/ 115] time: 2.0156s, d_loss: 1.024, g_loss: 3.030, rnn_loss: 0.149\n",
      "Epoch: [17/100] [ 100/ 115] time: 2.0469s, d_loss: 0.793, g_loss: 3.608, rnn_loss: 0.302\n",
      "Epoch: [18/100] [   0/ 115] time: 1.9831s, d_loss: 1.037, g_loss: 2.610, rnn_loss: 0.149\n",
      "Epoch: [18/100] [  50/ 115] time: 2.0609s, d_loss: 0.825, g_loss: 2.967, rnn_loss: 0.125\n",
      "Epoch: [18/100] [ 100/ 115] time: 1.9844s, d_loss: 0.811, g_loss: 3.566, rnn_loss: 0.134\n",
      "Epoch: [19/100] [   0/ 115] time: 2.0000s, d_loss: 0.972, g_loss: 3.576, rnn_loss: 0.149\n",
      "Epoch: [19/100] [  50/ 115] time: 2.0156s, d_loss: 0.881, g_loss: 3.046, rnn_loss: 0.131\n",
      "Epoch: [19/100] [ 100/ 115] time: 1.9923s, d_loss: 0.789, g_loss: 4.283, rnn_loss: 0.133\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [20/100] [   0/ 115] time: 2.0875s, d_loss: 0.783, g_loss: 4.199, rnn_loss: 0.101\n",
      "Epoch: [20/100] [  50/ 115] time: 2.0781s, d_loss: 0.912, g_loss: 3.503, rnn_loss: 0.143\n",
      "Epoch: [20/100] [ 100/ 115] time: 2.0625s, d_loss: 0.722, g_loss: 3.741, rnn_loss: 0.096\n",
      "Epoch: [21/100] [   0/ 115] time: 2.0156s, d_loss: 0.892, g_loss: 3.151, rnn_loss: 0.111\n",
      "Epoch: [21/100] [  50/ 115] time: 2.0037s, d_loss: 0.873, g_loss: 3.051, rnn_loss: 0.169\n",
      "Epoch: [21/100] [ 100/ 115] time: 2.0156s, d_loss: 0.783, g_loss: 3.258, rnn_loss: 0.126\n",
      "Epoch: [22/100] [   0/ 115] time: 2.0510s, d_loss: 0.834, g_loss: 3.561, rnn_loss: 0.120\n",
      "Epoch: [22/100] [  50/ 115] time: 2.0834s, d_loss: 0.876, g_loss: 2.947, rnn_loss: 0.133\n",
      "Epoch: [22/100] [ 100/ 115] time: 1.9844s, d_loss: 0.819, g_loss: 3.389, rnn_loss: 0.143\n",
      "Epoch: [23/100] [   0/ 115] time: 2.0393s, d_loss: 0.786, g_loss: 3.224, rnn_loss: 0.108\n",
      "Epoch: [23/100] [  50/ 115] time: 2.0550s, d_loss: 0.790, g_loss: 3.322, rnn_loss: 0.132\n",
      "Epoch: [23/100] [ 100/ 115] time: 1.9687s, d_loss: 0.774, g_loss: 3.440, rnn_loss: 0.111\n",
      "Epoch: [24/100] [   0/ 115] time: 2.0312s, d_loss: 0.731, g_loss: 3.796, rnn_loss: 0.093\n",
      "Epoch: [24/100] [  50/ 115] time: 2.0312s, d_loss: 0.758, g_loss: 3.741, rnn_loss: 0.121\n",
      "Epoch: [24/100] [ 100/ 115] time: 2.0469s, d_loss: 0.765, g_loss: 5.978, rnn_loss: 0.113\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [25/100] [   0/ 115] time: 2.0770s, d_loss: 0.799, g_loss: 3.108, rnn_loss: 0.090\n",
      "Epoch: [25/100] [  50/ 115] time: 2.0469s, d_loss: 0.815, g_loss: 3.218, rnn_loss: 0.125\n",
      "Epoch: [25/100] [ 100/ 115] time: 2.0094s, d_loss: 0.852, g_loss: 4.212, rnn_loss: 0.093\n",
      "Epoch: [26/100] [   0/ 115] time: 2.0469s, d_loss: 0.834, g_loss: 3.643, rnn_loss: 0.091\n",
      "Epoch: [26/100] [  50/ 115] time: 2.0781s, d_loss: 0.797, g_loss: 3.226, rnn_loss: 0.105\n",
      "Epoch: [26/100] [ 100/ 115] time: 2.0576s, d_loss: 0.795, g_loss: 5.489, rnn_loss: 0.098\n",
      "Epoch: [27/100] [   0/ 115] time: 2.0312s, d_loss: 0.640, g_loss: 4.114, rnn_loss: 0.085\n",
      "Epoch: [27/100] [  50/ 115] time: 2.0220s, d_loss: 0.837, g_loss: 2.692, rnn_loss: 0.121\n",
      "Epoch: [27/100] [ 100/ 115] time: 2.0511s, d_loss: 0.875, g_loss: 3.077, rnn_loss: 0.127\n",
      "Epoch: [28/100] [   0/ 115] time: 2.0097s, d_loss: 0.577, g_loss: 3.797, rnn_loss: 0.069\n",
      "Epoch: [28/100] [  50/ 115] time: 2.0798s, d_loss: 0.921, g_loss: 1.939, rnn_loss: 0.147\n",
      "Epoch: [28/100] [ 100/ 115] time: 2.0000s, d_loss: 0.582, g_loss: 3.643, rnn_loss: 0.079\n",
      "Epoch: [29/100] [   0/ 115] time: 2.0312s, d_loss: 0.862, g_loss: 1.703, rnn_loss: 0.072\n",
      "Epoch: [29/100] [  50/ 115] time: 2.0000s, d_loss: 0.797, g_loss: 2.481, rnn_loss: 0.175\n",
      "Epoch: [29/100] [ 100/ 115] time: 2.0156s, d_loss: 0.766, g_loss: 3.962, rnn_loss: 0.119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n",
      "Epoch: [30/100] [   0/ 115] time: 2.0258s, d_loss: 0.656, g_loss: 4.463, rnn_loss: 0.095\n",
      "Epoch: [30/100] [  50/ 115] time: 2.0144s, d_loss: 0.703, g_loss: 2.366, rnn_loss: 0.079\n",
      "Epoch: [30/100] [ 100/ 115] time: 2.1015s, d_loss: 0.792, g_loss: 3.234, rnn_loss: 0.103\n",
      "Epoch: [31/100] [   0/ 115] time: 1.9864s, d_loss: 0.740, g_loss: 3.830, rnn_loss: 0.086\n",
      "Epoch: [31/100] [  50/ 115] time: 2.0781s, d_loss: 0.570, g_loss: 3.944, rnn_loss: 0.094\n",
      "Epoch: [31/100] [ 100/ 115] time: 2.0291s, d_loss: 0.948, g_loss: 3.296, rnn_loss: 0.111\n",
      "Epoch: [32/100] [   0/ 115] time: 2.0000s, d_loss: 0.732, g_loss: 2.791, rnn_loss: 0.072\n",
      "Epoch: [32/100] [  50/ 115] time: 2.1043s, d_loss: 0.597, g_loss: 3.549, rnn_loss: 0.062\n",
      "Epoch: [32/100] [ 100/ 115] time: 1.9844s, d_loss: 0.723, g_loss: 2.926, rnn_loss: 0.092\n",
      "Epoch: [33/100] [   0/ 115] time: 2.0000s, d_loss: 0.716, g_loss: 3.165, rnn_loss: 0.093\n",
      "Epoch: [33/100] [  50/ 115] time: 2.0312s, d_loss: 1.019, g_loss: 3.853, rnn_loss: 0.051\n",
      "Epoch: [33/100] [ 100/ 115] time: 2.0091s, d_loss: 0.755, g_loss: 5.486, rnn_loss: 0.098\n",
      "Epoch: [34/100] [   0/ 115] time: 1.9932s, d_loss: 0.714, g_loss: 4.349, rnn_loss: 0.119\n",
      "Epoch: [34/100] [  50/ 115] time: 2.0864s, d_loss: 4.267, g_loss: 1.954, rnn_loss: 0.027\n",
      "Epoch: [34/100] [ 100/ 115] time: 2.0079s, d_loss: 0.812, g_loss: 5.717, rnn_loss: 0.107\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [35/100] [   0/ 115] time: 2.0473s, d_loss: 0.943, g_loss: 3.471, rnn_loss: 0.130\n",
      "Epoch: [35/100] [  50/ 115] time: 2.0156s, d_loss: 1.012, g_loss: 4.244, rnn_loss: 0.025\n",
      "Epoch: [35/100] [ 100/ 115] time: 2.0330s, d_loss: 0.740, g_loss: 5.541, rnn_loss: 0.130\n",
      "Epoch: [36/100] [   0/ 115] time: 2.0312s, d_loss: 0.793, g_loss: 2.662, rnn_loss: 0.118\n",
      "Epoch: [36/100] [  50/ 115] time: 2.0705s, d_loss: 1.677, g_loss: 1.916, rnn_loss: 0.032\n",
      "Epoch: [36/100] [ 100/ 115] time: 2.0112s, d_loss: 1.013, g_loss: 3.669, rnn_loss: 0.119\n",
      "Epoch: [37/100] [   0/ 115] time: 1.9926s, d_loss: 0.769, g_loss: 2.954, rnn_loss: 0.133\n",
      "Epoch: [37/100] [  50/ 115] time: 2.0156s, d_loss: 0.843, g_loss: 3.840, rnn_loss: 0.042\n",
      "Epoch: [37/100] [ 100/ 115] time: 2.0087s, d_loss: 0.708, g_loss: 3.700, rnn_loss: 0.109\n",
      "Epoch: [38/100] [   0/ 115] time: 2.0000s, d_loss: 0.742, g_loss: 3.749, rnn_loss: 0.115\n",
      "Epoch: [38/100] [  50/ 115] time: 1.9929s, d_loss: 1.055, g_loss: 3.749, rnn_loss: 0.073\n",
      "Epoch: [38/100] [ 100/ 115] time: 2.0469s, d_loss: 0.827, g_loss: 3.574, rnn_loss: 0.110\n",
      "Epoch: [39/100] [   0/ 115] time: 1.9844s, d_loss: 0.752, g_loss: 3.838, rnn_loss: 0.107\n",
      "Epoch: [39/100] [  50/ 115] time: 2.0000s, d_loss: 0.991, g_loss: 5.736, rnn_loss: 0.057\n",
      "Epoch: [39/100] [ 100/ 115] time: 1.9844s, d_loss: 0.749, g_loss: 4.815, rnn_loss: 0.116\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [40/100] [   0/ 115] time: 2.0683s, d_loss: 0.817, g_loss: 4.157, rnn_loss: 0.097\n",
      "Epoch: [40/100] [  50/ 115] time: 2.0000s, d_loss: 0.903, g_loss: 4.508, rnn_loss: 0.084\n",
      "Epoch: [40/100] [ 100/ 115] time: 2.0469s, d_loss: 0.663, g_loss: 3.060, rnn_loss: 0.082\n",
      "Epoch: [41/100] [   0/ 115] time: 1.9552s, d_loss: 0.850, g_loss: 3.065, rnn_loss: 0.075\n",
      "Epoch: [41/100] [  50/ 115] time: 1.9860s, d_loss: 1.207, g_loss: 3.325, rnn_loss: 0.120\n",
      "Epoch: [41/100] [ 100/ 115] time: 1.9844s, d_loss: 0.655, g_loss: 3.838, rnn_loss: 0.097\n",
      "Epoch: [42/100] [   0/ 115] time: 2.0000s, d_loss: 0.768, g_loss: 4.040, rnn_loss: 0.116\n",
      "Epoch: [42/100] [  50/ 115] time: 2.0364s, d_loss: 0.933, g_loss: 2.270, rnn_loss: 0.072\n",
      "Epoch: [42/100] [ 100/ 115] time: 1.9809s, d_loss: 1.446, g_loss: 1.018, rnn_loss: 0.120\n",
      "Epoch: [43/100] [   0/ 115] time: 2.0288s, d_loss: 0.695, g_loss: 3.405, rnn_loss: 0.088\n",
      "Epoch: [43/100] [  50/ 115] time: 1.9537s, d_loss: 0.754, g_loss: 3.270, rnn_loss: 0.089\n",
      "Epoch: [43/100] [ 100/ 115] time: 2.0186s, d_loss: 0.847, g_loss: 2.062, rnn_loss: 0.109\n",
      "Epoch: [44/100] [   0/ 115] time: 1.9844s, d_loss: 0.631, g_loss: 3.541, rnn_loss: 0.091\n",
      "Epoch: [44/100] [  50/ 115] time: 2.0000s, d_loss: 0.929, g_loss: 3.040, rnn_loss: 0.054\n",
      "Epoch: [44/100] [ 100/ 115] time: 1.9844s, d_loss: 0.717, g_loss: 2.916, rnn_loss: 0.106\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [45/100] [   0/ 115] time: 2.0838s, d_loss: 0.600, g_loss: 4.406, rnn_loss: 0.065\n",
      "Epoch: [45/100] [  50/ 115] time: 2.0000s, d_loss: 0.643, g_loss: 5.060, rnn_loss: 0.051\n",
      "Epoch: [45/100] [ 100/ 115] time: 2.0132s, d_loss: 0.742, g_loss: 4.053, rnn_loss: 0.119\n",
      "Epoch: [46/100] [   0/ 115] time: 1.9687s, d_loss: 0.750, g_loss: 2.971, rnn_loss: 0.099\n",
      "Epoch: [46/100] [  50/ 115] time: 1.9844s, d_loss: 0.386, g_loss: 4.774, rnn_loss: 0.037\n",
      "Epoch: [46/100] [ 100/ 115] time: 1.9946s, d_loss: 0.744, g_loss: 3.727, rnn_loss: 0.094\n",
      "Epoch: [47/100] [   0/ 115] time: 1.9687s, d_loss: 0.853, g_loss: 3.146, rnn_loss: 0.084\n",
      "Epoch: [47/100] [  50/ 115] time: 2.0000s, d_loss: 0.385, g_loss: 5.027, rnn_loss: 0.048\n",
      "Epoch: [47/100] [ 100/ 115] time: 1.9895s, d_loss: 0.847, g_loss: 3.686, rnn_loss: 0.097\n",
      "Epoch: [48/100] [   0/ 115] time: 2.0397s, d_loss: 0.676, g_loss: 3.059, rnn_loss: 0.071\n",
      "Epoch: [48/100] [  50/ 115] time: 1.9687s, d_loss: 0.349, g_loss: 5.299, rnn_loss: 0.041\n",
      "Epoch: [48/100] [ 100/ 115] time: 1.9901s, d_loss: 0.718, g_loss: 3.792, rnn_loss: 0.134\n",
      "Epoch: [49/100] [   0/ 115] time: 2.0286s, d_loss: 0.586, g_loss: 4.897, rnn_loss: 0.063\n",
      "Epoch: [49/100] [  50/ 115] time: 2.0312s, d_loss: 0.383, g_loss: 6.420, rnn_loss: 0.043\n",
      "Epoch: [49/100] [ 100/ 115] time: 1.9995s, d_loss: 0.690, g_loss: 3.918, rnn_loss: 0.103\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [50/100] [   0/ 115] time: 2.0311s, d_loss: 0.512, g_loss: 5.744, rnn_loss: 0.095\n",
      "Epoch: [50/100] [  50/ 115] time: 1.9887s, d_loss: 0.364, g_loss: 5.836, rnn_loss: 0.042\n",
      "Epoch: [50/100] [ 100/ 115] time: 2.0000s, d_loss: 1.293, g_loss: 2.274, rnn_loss: 0.089\n",
      "Epoch: [51/100] [   0/ 115] time: 2.0000s, d_loss: 0.687, g_loss: 4.263, rnn_loss: 0.102\n",
      "Epoch: [51/100] [  50/ 115] time: 2.0312s, d_loss: 0.504, g_loss: 4.665, rnn_loss: 0.058\n",
      "Epoch: [51/100] [ 100/ 115] time: 2.0591s, d_loss: 0.772, g_loss: 4.053, rnn_loss: 0.091\n",
      "Epoch: [52/100] [   0/ 115] time: 2.0312s, d_loss: 0.495, g_loss: 5.460, rnn_loss: 0.059\n",
      "Epoch: [52/100] [  50/ 115] time: 2.0720s, d_loss: 0.419, g_loss: 5.404, rnn_loss: 0.049\n",
      "Epoch: [52/100] [ 100/ 115] time: 2.0244s, d_loss: 1.197, g_loss: 3.998, rnn_loss: 0.085\n",
      "Epoch: [53/100] [   0/ 115] time: 1.9891s, d_loss: 0.693, g_loss: 3.095, rnn_loss: 0.077\n",
      "Epoch: [53/100] [  50/ 115] time: 2.1469s, d_loss: 0.431, g_loss: 6.791, rnn_loss: 0.048\n",
      "Epoch: [53/100] [ 100/ 115] time: 2.0357s, d_loss: 0.547, g_loss: 4.024, rnn_loss: 0.088\n",
      "Epoch: [54/100] [   0/ 115] time: 2.0717s, d_loss: 0.572, g_loss: 4.476, rnn_loss: 0.077\n",
      "Epoch: [54/100] [  50/ 115] time: 2.0469s, d_loss: 0.920, g_loss: 3.456, rnn_loss: 0.076\n",
      "Epoch: [54/100] [ 100/ 115] time: 2.0192s, d_loss: 0.497, g_loss: 3.948, rnn_loss: 0.091\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [55/100] [   0/ 115] time: 2.0832s, d_loss: 0.947, g_loss: 4.605, rnn_loss: 0.065\n",
      "Epoch: [55/100] [  50/ 115] time: 2.0312s, d_loss: 0.523, g_loss: 5.034, rnn_loss: 0.046\n",
      "Epoch: [55/100] [ 100/ 115] time: 2.0625s, d_loss: 0.454, g_loss: 4.701, rnn_loss: 0.068\n",
      "Epoch: [56/100] [   0/ 115] time: 2.0781s, d_loss: 0.566, g_loss: 5.347, rnn_loss: 0.074\n",
      "Epoch: [56/100] [  50/ 115] time: 2.0469s, d_loss: 0.387, g_loss: 3.912, rnn_loss: 0.027\n",
      "Epoch: [56/100] [ 100/ 115] time: 2.0312s, d_loss: 0.445, g_loss: 5.078, rnn_loss: 0.063\n",
      "Epoch: [57/100] [   0/ 115] time: 2.0059s, d_loss: 0.596, g_loss: 4.059, rnn_loss: 0.083\n",
      "Epoch: [57/100] [  50/ 115] time: 2.0312s, d_loss: 0.684, g_loss: 2.582, rnn_loss: 0.059\n",
      "Epoch: [57/100] [ 100/ 115] time: 2.0676s, d_loss: 0.504, g_loss: 4.865, rnn_loss: 0.081\n",
      "Epoch: [58/100] [   0/ 115] time: 2.0639s, d_loss: 0.499, g_loss: 4.067, rnn_loss: 0.045\n",
      "Epoch: [58/100] [  50/ 115] time: 2.0781s, d_loss: 0.465, g_loss: 4.840, rnn_loss: 0.054\n",
      "Epoch: [58/100] [ 100/ 115] time: 2.0190s, d_loss: 0.399, g_loss: 5.071, rnn_loss: 0.070\n",
      "Epoch: [59/100] [   0/ 115] time: 2.0312s, d_loss: 0.485, g_loss: 3.730, rnn_loss: 0.055\n",
      "Epoch: [59/100] [  50/ 115] time: 2.0937s, d_loss: 0.777, g_loss: 1.734, rnn_loss: 0.043\n",
      "Epoch: [59/100] [ 100/ 115] time: 2.0312s, d_loss: 0.434, g_loss: 4.643, rnn_loss: 0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n",
      "Epoch: [60/100] [   0/ 115] time: 2.0686s, d_loss: 0.749, g_loss: 2.804, rnn_loss: 0.091\n",
      "Epoch: [60/100] [  50/ 115] time: 2.0625s, d_loss: 0.599, g_loss: 4.693, rnn_loss: 0.074\n",
      "Epoch: [60/100] [ 100/ 115] time: 2.0485s, d_loss: 0.522, g_loss: 4.482, rnn_loss: 0.070\n",
      "Epoch: [61/100] [   0/ 115] time: 2.0625s, d_loss: 0.518, g_loss: 4.771, rnn_loss: 0.078\n",
      "Epoch: [61/100] [  50/ 115] time: 2.0937s, d_loss: 0.448, g_loss: 4.342, rnn_loss: 0.047\n",
      "Epoch: [61/100] [ 100/ 115] time: 2.0156s, d_loss: 0.515, g_loss: 3.346, rnn_loss: 0.057\n",
      "Epoch: [62/100] [   0/ 115] time: 2.0283s, d_loss: 0.448, g_loss: 4.260, rnn_loss: 0.070\n",
      "Epoch: [62/100] [  50/ 115] time: 2.0156s, d_loss: 0.620, g_loss: 7.180, rnn_loss: 0.053\n",
      "Epoch: [62/100] [ 100/ 115] time: 2.0469s, d_loss: 0.419, g_loss: 3.831, rnn_loss: 0.071\n",
      "Epoch: [63/100] [   0/ 115] time: 1.9687s, d_loss: 0.497, g_loss: 3.467, rnn_loss: 0.065\n",
      "Epoch: [63/100] [  50/ 115] time: 2.0469s, d_loss: 0.384, g_loss: 3.923, rnn_loss: 0.042\n",
      "Epoch: [63/100] [ 100/ 115] time: 2.0156s, d_loss: 0.423, g_loss: 4.087, rnn_loss: 0.062\n",
      "Epoch: [64/100] [   0/ 115] time: 2.0156s, d_loss: 0.434, g_loss: 4.499, rnn_loss: 0.060\n",
      "Epoch: [64/100] [  50/ 115] time: 2.0377s, d_loss: 0.415, g_loss: 5.913, rnn_loss: 0.036\n",
      "Epoch: [64/100] [ 100/ 115] time: 2.0093s, d_loss: 0.479, g_loss: 4.697, rnn_loss: 0.145\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [65/100] [   0/ 115] time: 2.0529s, d_loss: 0.445, g_loss: 7.493, rnn_loss: 0.054\n",
      "Epoch: [65/100] [  50/ 115] time: 2.0625s, d_loss: 0.632, g_loss: 2.482, rnn_loss: 0.043\n",
      "Epoch: [65/100] [ 100/ 115] time: 1.9985s, d_loss: 0.564, g_loss: 3.637, rnn_loss: 0.087\n",
      "Epoch: [66/100] [   0/ 115] time: 2.0469s, d_loss: 0.655, g_loss: 2.992, rnn_loss: 0.072\n",
      "Epoch: [66/100] [  50/ 115] time: 2.0625s, d_loss: 0.417, g_loss: 3.414, rnn_loss: 0.042\n",
      "Epoch: [66/100] [ 100/ 115] time: 2.0309s, d_loss: 0.598, g_loss: 6.539, rnn_loss: 0.069\n",
      "Epoch: [67/100] [   0/ 115] time: 2.0312s, d_loss: 0.505, g_loss: 3.634, rnn_loss: 0.067\n",
      "Epoch: [67/100] [  50/ 115] time: 2.0828s, d_loss: 0.857, g_loss: 5.820, rnn_loss: 0.042\n",
      "Epoch: [67/100] [ 100/ 115] time: 2.0156s, d_loss: 0.547, g_loss: 4.138, rnn_loss: 0.095\n",
      "Epoch: [68/100] [   0/ 115] time: 2.0000s, d_loss: 0.747, g_loss: 5.346, rnn_loss: 0.071\n",
      "Epoch: [68/100] [  50/ 115] time: 2.0653s, d_loss: 0.588, g_loss: 4.344, rnn_loss: 0.062\n",
      "Epoch: [68/100] [ 100/ 115] time: 2.0907s, d_loss: 0.543, g_loss: 3.986, rnn_loss: 0.067\n",
      "Epoch: [69/100] [   0/ 115] time: 2.0300s, d_loss: 0.525, g_loss: 3.348, rnn_loss: 0.060\n",
      "Epoch: [69/100] [  50/ 115] time: 2.0937s, d_loss: 0.481, g_loss: 3.051, rnn_loss: 0.046\n",
      "Epoch: [69/100] [ 100/ 115] time: 2.0441s, d_loss: 0.463, g_loss: 4.276, rnn_loss: 0.066\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [70/100] [   0/ 115] time: 2.0062s, d_loss: 0.475, g_loss: 5.485, rnn_loss: 0.070\n",
      "Epoch: [70/100] [  50/ 115] time: 2.0937s, d_loss: 0.421, g_loss: 4.411, rnn_loss: 0.046\n",
      "Epoch: [70/100] [ 100/ 115] time: 2.0826s, d_loss: 0.755, g_loss: 4.248, rnn_loss: 0.082\n",
      "Epoch: [71/100] [   0/ 115] time: 2.0525s, d_loss: 1.140, g_loss: 3.799, rnn_loss: 0.080\n",
      "Epoch: [71/100] [  50/ 115] time: 2.0312s, d_loss: 0.454, g_loss: 3.964, rnn_loss: 0.058\n",
      "Epoch: [71/100] [ 100/ 115] time: 2.0156s, d_loss: 0.518, g_loss: 4.190, rnn_loss: 0.067\n",
      "Epoch: [72/100] [   0/ 115] time: 2.0312s, d_loss: 0.444, g_loss: 3.797, rnn_loss: 0.062\n",
      "Epoch: [72/100] [  50/ 115] time: 2.0241s, d_loss: 0.423, g_loss: 3.044, rnn_loss: 0.032\n",
      "Epoch: [72/100] [ 100/ 115] time: 2.0017s, d_loss: 0.548, g_loss: 4.867, rnn_loss: 0.062\n",
      "Epoch: [73/100] [   0/ 115] time: 1.9844s, d_loss: 0.512, g_loss: 4.086, rnn_loss: 0.037\n",
      "Epoch: [73/100] [  50/ 115] time: 2.0312s, d_loss: 0.641, g_loss: 2.825, rnn_loss: 0.057\n",
      "Epoch: [73/100] [ 100/ 115] time: 2.0000s, d_loss: 0.517, g_loss: 3.925, rnn_loss: 0.046\n",
      "Epoch: [74/100] [   0/ 115] time: 2.0156s, d_loss: 1.419, g_loss: 3.801, rnn_loss: 0.053\n",
      "Epoch: [74/100] [  50/ 115] time: 1.9844s, d_loss: 0.802, g_loss: 2.534, rnn_loss: 0.104\n",
      "Epoch: [74/100] [ 100/ 115] time: 2.0289s, d_loss: 0.432, g_loss: 3.971, rnn_loss: 0.049\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [75/100] [   0/ 115] time: 2.0620s, d_loss: 1.011, g_loss: 4.444, rnn_loss: 0.037\n",
      "Epoch: [75/100] [  50/ 115] time: 2.0625s, d_loss: 0.878, g_loss: 1.867, rnn_loss: 0.077\n",
      "Epoch: [75/100] [ 100/ 115] time: 2.0156s, d_loss: 0.388, g_loss: 3.821, rnn_loss: 0.038\n",
      "Epoch: [76/100] [   0/ 115] time: 2.0000s, d_loss: 0.609, g_loss: 5.332, rnn_loss: 0.049\n",
      "Epoch: [76/100] [  50/ 115] time: 2.0312s, d_loss: 0.998, g_loss: 3.121, rnn_loss: 0.119\n",
      "Epoch: [76/100] [ 100/ 115] time: 2.1248s, d_loss: 0.461, g_loss: 4.303, rnn_loss: 0.051\n",
      "Epoch: [77/100] [   0/ 115] time: 2.1350s, d_loss: 0.675, g_loss: 4.844, rnn_loss: 0.073\n",
      "Epoch: [77/100] [  50/ 115] time: 2.0781s, d_loss: 0.677, g_loss: 3.899, rnn_loss: 0.070\n",
      "Epoch: [77/100] [ 100/ 115] time: 2.0173s, d_loss: 0.366, g_loss: 5.438, rnn_loss: 0.047\n",
      "Epoch: [78/100] [   0/ 115] time: 2.0469s, d_loss: 0.367, g_loss: 4.129, rnn_loss: 0.062\n",
      "Epoch: [78/100] [  50/ 115] time: 2.0469s, d_loss: 0.412, g_loss: 3.559, rnn_loss: 0.072\n",
      "Epoch: [78/100] [ 100/ 115] time: 2.1304s, d_loss: 0.372, g_loss: 4.307, rnn_loss: 0.046\n",
      "Epoch: [79/100] [   0/ 115] time: 2.0309s, d_loss: 0.417, g_loss: 5.538, rnn_loss: 0.063\n",
      "Epoch: [79/100] [  50/ 115] time: 2.0312s, d_loss: 0.465, g_loss: 4.590, rnn_loss: 0.073\n",
      "Epoch: [79/100] [ 100/ 115] time: 2.1474s, d_loss: 0.357, g_loss: 4.174, rnn_loss: 0.055\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [80/100] [   0/ 115] time: 2.0833s, d_loss: 0.620, g_loss: 5.752, rnn_loss: 0.059\n",
      "Epoch: [80/100] [  50/ 115] time: 2.0937s, d_loss: 0.355, g_loss: 4.717, rnn_loss: 0.048\n",
      "Epoch: [80/100] [ 100/ 115] time: 2.0000s, d_loss: 0.346, g_loss: 5.773, rnn_loss: 0.041\n",
      "Epoch: [81/100] [   0/ 115] time: 2.0312s, d_loss: 0.552, g_loss: 4.144, rnn_loss: 0.069\n",
      "Epoch: [81/100] [  50/ 115] time: 2.0204s, d_loss: 0.396, g_loss: 4.660, rnn_loss: 0.076\n",
      "Epoch: [81/100] [ 100/ 115] time: 2.0000s, d_loss: 0.519, g_loss: 6.890, rnn_loss: 0.039\n",
      "Epoch: [82/100] [   0/ 115] time: 2.0608s, d_loss: 0.415, g_loss: 4.771, rnn_loss: 0.059\n",
      "Epoch: [82/100] [  50/ 115] time: 2.0312s, d_loss: 0.355, g_loss: 6.884, rnn_loss: 0.053\n",
      "Epoch: [82/100] [ 100/ 115] time: 2.0000s, d_loss: 0.351, g_loss: 4.417, rnn_loss: 0.042\n",
      "Epoch: [83/100] [   0/ 115] time: 2.0469s, d_loss: 0.437, g_loss: 4.784, rnn_loss: 0.063\n",
      "Epoch: [83/100] [  50/ 115] time: 2.0312s, d_loss: 0.420, g_loss: 5.903, rnn_loss: 0.077\n",
      "Epoch: [83/100] [ 100/ 115] time: 1.9844s, d_loss: 0.282, g_loss: 5.134, rnn_loss: 0.024\n",
      "Epoch: [84/100] [   0/ 115] time: 2.0000s, d_loss: 0.509, g_loss: 4.142, rnn_loss: 0.080\n",
      "Epoch: [84/100] [  50/ 115] time: 2.1094s, d_loss: 0.314, g_loss: 5.025, rnn_loss: 0.053\n",
      "Epoch: [84/100] [ 100/ 115] time: 2.0073s, d_loss: 0.457, g_loss: 6.160, rnn_loss: 0.035\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [85/100] [   0/ 115] time: 2.0209s, d_loss: 0.346, g_loss: 3.879, rnn_loss: 0.049\n",
      "Epoch: [85/100] [  50/ 115] time: 2.1573s, d_loss: 0.573, g_loss: 5.201, rnn_loss: 0.075\n",
      "Epoch: [85/100] [ 100/ 115] time: 2.0871s, d_loss: 0.442, g_loss: 4.259, rnn_loss: 0.062\n",
      "Epoch: [86/100] [   0/ 115] time: 2.0156s, d_loss: 0.279, g_loss: 4.798, rnn_loss: 0.035\n",
      "Epoch: [86/100] [  50/ 115] time: 2.0469s, d_loss: 0.415, g_loss: 5.198, rnn_loss: 0.065\n",
      "Epoch: [86/100] [ 100/ 115] time: 2.0209s, d_loss: 0.437, g_loss: 4.430, rnn_loss: 0.056\n",
      "Epoch: [87/100] [   0/ 115] time: 2.0781s, d_loss: 0.342, g_loss: 4.091, rnn_loss: 0.046\n",
      "Epoch: [87/100] [  50/ 115] time: 2.0469s, d_loss: 0.318, g_loss: 4.178, rnn_loss: 0.068\n",
      "Epoch: [87/100] [ 100/ 115] time: 2.0156s, d_loss: 0.456, g_loss: 4.359, rnn_loss: 0.073\n",
      "Epoch: [88/100] [   0/ 115] time: 1.9844s, d_loss: 0.361, g_loss: 4.634, rnn_loss: 0.058\n",
      "Epoch: [88/100] [  50/ 115] time: 2.0077s, d_loss: 0.391, g_loss: 4.612, rnn_loss: 0.060\n",
      "Epoch: [88/100] [ 100/ 115] time: 1.9844s, d_loss: 0.599, g_loss: 4.592, rnn_loss: 0.072\n",
      "Epoch: [89/100] [   0/ 115] time: 2.0000s, d_loss: 0.333, g_loss: 4.233, rnn_loss: 0.056\n",
      "Epoch: [89/100] [  50/ 115] time: 2.0000s, d_loss: 0.345, g_loss: 3.447, rnn_loss: 0.036\n",
      "Epoch: [89/100] [ 100/ 115] time: 2.1094s, d_loss: 0.351, g_loss: 4.536, rnn_loss: 0.071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----success saved checkpoint--------\n",
      "Epoch: [90/100] [   0/ 115] time: 1.8658s, d_loss: 0.359, g_loss: 3.828, rnn_loss: 0.000\n",
      "Epoch: [90/100] [  50/ 115] time: 1.8437s, d_loss: 0.422, g_loss: 3.687, rnn_loss: 0.000\n",
      "Epoch: [90/100] [ 100/ 115] time: 1.9248s, d_loss: 0.431, g_loss: 3.935, rnn_loss: 0.000\n",
      "Epoch: [91/100] [   0/ 115] time: 1.8594s, d_loss: 0.413, g_loss: 4.087, rnn_loss: 0.000\n",
      "Epoch: [91/100] [  50/ 115] time: 1.8780s, d_loss: 0.453, g_loss: 3.033, rnn_loss: 0.000\n",
      "Epoch: [91/100] [ 100/ 115] time: 1.9044s, d_loss: 0.388, g_loss: 5.514, rnn_loss: 0.000\n",
      "Epoch: [92/100] [   0/ 115] time: 1.8594s, d_loss: 1.452, g_loss: 5.117, rnn_loss: 0.000\n",
      "Epoch: [92/100] [  50/ 115] time: 1.8786s, d_loss: 0.397, g_loss: 3.462, rnn_loss: 0.000\n",
      "Epoch: [92/100] [ 100/ 115] time: 1.8437s, d_loss: 0.395, g_loss: 3.492, rnn_loss: 0.000\n",
      "Epoch: [93/100] [   0/ 115] time: 1.8281s, d_loss: 0.483, g_loss: 4.609, rnn_loss: 0.000\n",
      "Epoch: [93/100] [  50/ 115] time: 1.8393s, d_loss: 0.444, g_loss: 3.861, rnn_loss: 0.000\n",
      "Epoch: [93/100] [ 100/ 115] time: 1.8503s, d_loss: 0.321, g_loss: 4.523, rnn_loss: 0.000\n",
      "Epoch: [94/100] [   0/ 115] time: 1.8594s, d_loss: 0.387, g_loss: 4.207, rnn_loss: 0.000\n",
      "Epoch: [94/100] [  50/ 115] time: 1.8750s, d_loss: 1.172, g_loss: 4.323, rnn_loss: 0.000\n",
      "Epoch: [94/100] [ 100/ 115] time: 1.8454s, d_loss: 0.401, g_loss: 4.972, rnn_loss: 0.000\n",
      "-----success saved checkpoint--------\n",
      "Epoch: [95/100] [   0/ 115] time: 1.9179s, d_loss: 0.412, g_loss: 5.162, rnn_loss: 0.000\n",
      "Epoch: [95/100] [  50/ 115] time: 1.8281s, d_loss: 0.398, g_loss: 3.288, rnn_loss: 0.000\n",
      "Epoch: [95/100] [ 100/ 115] time: 1.8437s, d_loss: 0.390, g_loss: 4.721, rnn_loss: 0.000\n",
      "Epoch: [96/100] [   0/ 115] time: 1.8437s, d_loss: 0.365, g_loss: 3.620, rnn_loss: 0.000\n",
      "Epoch: [96/100] [  50/ 115] time: 1.8906s, d_loss: 0.535, g_loss: 2.693, rnn_loss: 0.000\n",
      "Epoch: [96/100] [ 100/ 115] time: 1.8766s, d_loss: 0.413, g_loss: 3.830, rnn_loss: 0.000\n",
      "Epoch: [97/100] [   0/ 115] time: 1.8142s, d_loss: 0.340, g_loss: 4.956, rnn_loss: 0.000\n",
      "Epoch: [97/100] [  50/ 115] time: 1.8437s, d_loss: 0.543, g_loss: 4.177, rnn_loss: 0.000\n",
      "Epoch: [97/100] [ 100/ 115] time: 1.8437s, d_loss: 0.357, g_loss: 3.246, rnn_loss: 0.000\n",
      "Epoch: [98/100] [   0/ 115] time: 1.8281s, d_loss: 0.489, g_loss: 4.455, rnn_loss: 0.000\n",
      "Epoch: [98/100] [  50/ 115] time: 1.8281s, d_loss: 0.486, g_loss: 4.317, rnn_loss: 0.000\n",
      "Epoch: [98/100] [ 100/ 115] time: 1.8125s, d_loss: 0.354, g_loss: 4.493, rnn_loss: 0.000\n",
      "Epoch: [99/100] [   0/ 115] time: 1.8437s, d_loss: 0.378, g_loss: 4.808, rnn_loss: 0.000\n",
      "Epoch: [99/100] [  50/ 115] time: 1.8281s, d_loss: 0.473, g_loss: 4.201, rnn_loss: 0.000\n",
      "Epoch: [99/100] [ 100/ 115] time: 1.8285s, d_loss: 0.565, g_loss: 3.953, rnn_loss: 0.000\n",
      "-----success saved checkpoint--------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "checkpoint_path = './checkpoint/'\n",
    "inference_path = './inference'\n",
    "gan = GAN(get_hparas(), training_phase=True, dataset_path=data_path, ckpt_path=checkpoint_path, inference_path=inference_path)\n",
    "gan.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "checkpoint_path = './checkpoint/'\n",
    "inference_path = './inference'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator_test(filenames, batch_size):\n",
    "    data = pd.read_pickle(filenames)\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    \n",
    "    return iterator, output_types, output_shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "iterator_train, types, shapes = data_iterator_test(data_path+'/testData.pkl', 64)\n",
    "iter_initializer = iterator_train.initializer\n",
    "next_element = iterator_train.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator_train.initializer)\n",
    "    next_element = iterator_train.get_next()\n",
    "    caption, idex = sess.run(next_element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/rnn_model_79.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/g_model_79.ckpt\n",
      "-----success restored checkpoint--------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "gan = GAN(get_hparas(), training_phase=False, dataset_path=data_path, ckpt_path=checkpoint_path, inference_path=inference_path, recover=79)\n",
    "img = gan.inference()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)聽完這次的比賽分享，他們一樣有使用到DCGAN的架構，主要跟分享者的差別就是model蓋得不夠深，training的epoch也遠遠多過我們的數量。\n",
    "\n",
    "(2)在model output出來的值域也沒有改成跟tanh同樣的值域，導致圖片生出來的顏色都比較淺。\n",
    "\n",
    "(3)也有組別嘗試了其他架構，比如：Stack-GAN，以及Virtual Bacth Normalization等等技巧，來得到更好的效果，這樣不斷嘗試的苦功真的是我們需要好好學習的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 備註： 我們的test demo在另外一個檔裡面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
