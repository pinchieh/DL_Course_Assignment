{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "# load lines dictionary \n",
    "lines = open('dataset/chatbot/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "# load conversations\n",
    "convs= open('dataset/chatbot/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "print('\\n'.join(lines[:3]))\n",
    "print()\n",
    "print('\\n'.join(convs[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Text preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)把縮寫還原，並去掉特殊符號與標點符號。太長的句子與出現頻率不高的字詞丟掉。最後把token與字詞轉換成integer，寫進dictionary 存起來以利之後\n",
    "\n",
    "2)在每句都加上token並把不在dictionary內的字詞替換成UNK，在最後要丟進去model時，把含有UNK的句子丟掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_conv = {}\n",
    "for line in lines:\n",
    "    line_split = line.split(' +++$+++ ')\n",
    "    if len(line_split) == 5:\n",
    "        id_conv[line_split[0]] = line_split[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs_idlist = [ ]\n",
    "\n",
    "for line in convs[:]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\").split(',')\n",
    "    convs_idlist.append(_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs_idlist:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id_conv[conv[i]])\n",
    "        answers.append(id_conv[conv[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_line_length = 2\n",
    "max_line_length = 15\n",
    "\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['<PAD>','<END>','<UNK>','<BEG>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "##store word_to_int dictionary\n",
    "np.save('questions_vocab_to_int.npy',questions_vocab_to_int)\n",
    "np.save('answers_vocab_to_int.npy',answers_vocab_to_int)\n",
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}\n",
    "np.save('questions_int_to_vocab.npy', questions_int_to_vocab)\n",
    "np.save('answers_int_to_vocab.npy', answers_int_to_vocab)\n",
    "#print(questions_int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the end of sentence token to the end of every answer.\n",
    "ques_vocab = np.load('./questions_vocab_to_int.npy').tolist()\n",
    "ans_vocab = np.load('./answers_vocab_to_int.npy').tolist()\n",
    "ques_rev = np.load('./questions_int_to_vocab.npy').tolist()\n",
    "ans_rev = np.load('./answers_int_to_vocab.npy').tolist()\n",
    "\n",
    "for i in range(len(short_answers)):\n",
    "    short_answers[i] = '<BEG> '+short_answers[i]+' <END>'\n",
    "for i in range(len(short_questions)):\n",
    "    short_questions[i] = '<BEG> '+short_questions[i]+' <END>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the words to integers. \n",
    "# Replace the words that are not in the respective vocabulary with <UNK> \n",
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BEG> you are asking me out that is so cute that is your name again <END>\n",
      "<BEG> forget it <END>\n",
      "\n",
      "<BEG> gosh if only we could find kat a boyfriend <END>\n",
      "<BEG> let me see what i can do <END>\n",
      "\n",
      "<BEG> that is because it is such a nice one <END>\n",
      "<BEG> forget french <END>\n",
      "\n",
      "<BEG> you have my word as a gentleman <END>\n",
      "<BEG> you are sweet <END>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_corpus_clean = []\n",
    "ch_corpus_clean = []\n",
    "\n",
    "for i in range(len(questions_int)):\n",
    "    if not(ques_vocab['<UNK>'] in questions_int[i] or ans_vocab['<UNK>'] in answers_int[i]): # remove '<UNK>' sentence\n",
    "        en_corpus_clean.append(questions_int[i])\n",
    "        ch_corpus_clean.append(answers_int[i])\n",
    "for i in range(4):\n",
    "    print(' '.join([ques_rev[en_corpus_clean[i][j]]  for j in range(len(en_corpus_clean[i]))]))\n",
    "    print(' '.join([ans_rev[ch_corpus_clean[i][j]]  for j in range(len(ch_corpus_clean[i]))]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 17\n"
     ]
    }
   ],
   "source": [
    "en_max_len = 0\n",
    "ch_max_len = 0\n",
    "\n",
    "for i in range(len(en_corpus_clean)): # caculate max length\n",
    "    en_max_len = max(en_max_len, len(en_corpus_clean[i]))\n",
    "    ch_max_len = max(ch_max_len, len(ch_corpus_clean[i]))\n",
    "\n",
    "print(en_max_len, ch_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Batch preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿用LAB notebook的data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input\n",
      "Decoder input\n",
      "Decoder output\n",
      "\n",
      "do you listen to this crap <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> what crap <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "what crap <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "i figured you would get to the good stuff eventually <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> what good stuff <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "what good stuff <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "what good stuff <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> the real you <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "the real you <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "she okay <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<BEG> i hope so <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "i hope so <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, en_corpus, ch_corpus, en_pad, ch_pad, en_max_len, ch_max_len, batch_size):\n",
    "        assert len(en_corpus) == len(ch_corpus)\n",
    "        \n",
    "        batch_num = len(en_corpus)//batch_size\n",
    "        n = batch_num*batch_size\n",
    "        \n",
    "        self.xs = [np.zeros(n, dtype=np.int32) for _ in range(en_max_len)] # encoder inputs\n",
    "        self.ys = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder inputs\n",
    "        self.gs = [np.zeros(n, dtype=np.int32) for _ in range(ch_max_len)] # decoder outputs\n",
    "        self.ws = [np.zeros(n, dtype=np.float32) for _ in range(ch_max_len)] # decoder weight for loss caculation\n",
    "        \n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for b in range(batch_num):\n",
    "            for i in range(b*batch_size, (b+1)*batch_size):\n",
    "                for j in range(len(en_corpus[i])-2):\n",
    "                    self.xs[j][i] = en_corpus[i][j+1]\n",
    "                for j in range(j+1, en_max_len):\n",
    "                    self.xs[j][i] = en_pad\n",
    "                \n",
    "                for j in range(len(ch_corpus[i])-1):\n",
    "                    self.ys[j][i] = ch_corpus[i][j]\n",
    "                    self.gs[j][i] = ch_corpus[i][j+1]\n",
    "                    self.ws[j][i] = 1.0\n",
    "                for j in range(j+1, ch_max_len): # don't forget padding and let loss weight zero\n",
    "                    self.ys[j][i] = ch_pad\n",
    "                    self.gs[j][i] = ch_pad\n",
    "                    self.ws[j][i] = 0.0\n",
    "    \n",
    "    def get(self, batch_id):\n",
    "        x = [self.xs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.en_max_len)]\n",
    "        y = [self.ys[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        g = [self.gs[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        w = [self.ws[i][batch_id*self.batch_size:(batch_id+1)*self.batch_size] for i in range(self.ch_max_len)]\n",
    "        \n",
    "        return x, y, g, w\n",
    "\n",
    "batch = BatchGenerator(en_corpus_clean, ch_corpus_clean, \n",
    "                       ques_vocab['<PAD>'], ans_vocab['<PAD>'], en_max_len, ch_max_len, 4)\n",
    "\n",
    "print(\"Encoder input\")\n",
    "print(\"Decoder input\")\n",
    "print(\"Decoder output\")\n",
    "print()\n",
    "\n",
    "x, y, g, w = batch.get(2)\n",
    "for i in range(4):\n",
    "    print(' '.join([ques_rev[x[j][i]] for j in range(en_max_len)]))\n",
    "    print(' '.join([ans_rev[y[j][i]] for j in range(ch_max_len)]))\n",
    "    print(' '.join([ans_rev[g[j][i]] for j in range(ch_max_len)]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿用LAB notebook 上 seq2seq的model，train 100個 Epochs，最後用Cherry Pick看結果，前面輸出的結果問答對應都頗合適。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineTranslationSeq2Seq:\n",
    "    def __init__(self, en_max_len, ch_max_len, en_size, ch_size):\n",
    "        self.en_max_len = en_max_len\n",
    "        self.ch_max_len = ch_max_len\n",
    "        \n",
    "        with tf.variable_scope('seq2seq_intput/output'):\n",
    "            self.enc_inputs = [tf.placeholder(tf.int32, [None]) for i in range(en_max_len)] # time mojor feed\n",
    "            self.dec_inputs = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.groundtruths = [tf.placeholder(tf.int32, [None]) for i in range(ch_max_len)]\n",
    "            self.weights = [tf.placeholder(tf.float32, [None]) for i in range(ch_max_len)]\n",
    "            \n",
    "        with tf.variable_scope('seq2seq_rnn'): # training by teacher forcing\n",
    "            self.out_cell = tf.contrib.rnn.LSTMCell(512)\n",
    "            self.outputs, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                    self.out_cell, \n",
    "                                                                                    en_size, ch_size, 300)\n",
    "        with tf.variable_scope('seq2seq_rnn', reuse=True): # predict by feeding previous\n",
    "            self.pred_cell = tf.contrib.rnn.LSTMCell(512, reuse=True) # reuse cell for train and test\n",
    "            self.predictions, _ = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(self.enc_inputs, self.dec_inputs, \n",
    "                                                                                        self.pred_cell, \n",
    "                                                                                        en_size, ch_size, 300, \n",
    "                                                                                        feed_previous=True)\n",
    "        \n",
    "        with tf.variable_scope('loss'):\n",
    "            # caculate weighted loss\n",
    "            self.loss = tf.reduce_mean(tf.contrib.legacy_seq2seq.sequence_loss_by_example(self.outputs, \n",
    "                                                                                          self.groundtruths, \n",
    "                                                                                          self.weights))\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.002).minimize(self.loss)\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train(self, x, y, g, w):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i] # show how to feed a list\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "            fd[self.groundtruths[i]] = g[i]\n",
    "            fd[self.weights[i]] = w[i]\n",
    "        \n",
    "        loss, _ = self.sess.run([self.loss, self.optimizer], fd)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def output(self, x, y):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len):\n",
    "            fd[self.dec_inputs[i]] = y[i]\n",
    "        \n",
    "        out = self.sess.run(self.outputs, fd)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, x, ch_beg):\n",
    "        fd = {}\n",
    "        for i in range(self.en_max_len):\n",
    "            fd[self.enc_inputs[i]] = x[i]\n",
    "        \n",
    "        for i in range(self.ch_max_len): # when feed previous, the fist token should be '<BEG>', and others are useless\n",
    "            if i==0:\n",
    "                fd[self.dec_inputs[i]] = np.ones(y[i].shape, dtype=np.int32)*ch_beg\n",
    "            else:\n",
    "                fd[self.dec_inputs[i]] = np.zeros(y[i].shape, dtype=np.int32)\n",
    "        \n",
    "        pd = self.sess.run(self.predictions, fd)\n",
    "        \n",
    "        return pd\n",
    "    \n",
    "    def save(self, e):\n",
    "        self.saver.save(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e+1))\n",
    "    \n",
    "    def restore(self, e):\n",
    "        self.saver.restore(self.sess, 'model/seq2seq/seq2seq_%d.ckpt'%(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = MachineTranslationSeq2Seq(en_max_len, ch_max_len, len(ques_vocab), len(ans_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "batch_num = len(en_corpus_clean)//BATCH_SIZE\n",
    "\n",
    "batch = BatchGenerator(en_corpus_clean, ch_corpus_clean, \n",
    "                       ques_vocab['<PAD>'], ans_vocab['<PAD>'], \n",
    "                       en_max_len, ch_max_len, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrec_loss = []\\nfor e in range(EPOCHS):\\n    train_loss = 0\\n    \\n    for b in range(batch_num):\\n        x, y, g, w = batch.get(b)\\n        batch_loss = model.train(x, y, g, w)\\n        train_loss += batch_loss\\n    \\n    train_loss /= batch_num\\n    rec_loss.append(train_loss)\\n    print(\"epoch %d loss: %f\" % (e, train_loss))\\n    \\n    model.save(e)\\n    \\nnp.save(\\'./model/seq2seq/rec_loss.npy\\', rec_loss)\\n'"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_loss = []\n",
    "for e in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for b in range(batch_num):\n",
    "        x, y, g, w = batch.get(b)\n",
    "        batch_loss = model.train(x, y, g, w)\n",
    "        train_loss += batch_loss\n",
    "    \n",
    "    train_loss /= batch_num\n",
    "    rec_loss.append(train_loss)\n",
    "    print(\"epoch %d loss: %f\" % (e, train_loss))\n",
    "    \n",
    "    model.save(e)\n",
    "    \n",
    "np.save('./model/seq2seq/rec_loss.npy', rec_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/seq2seq/seq2seq_98.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.restore(98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def cherry_pick(records, n, upper_bound=1.0):\n",
    "    bleus = []\n",
    "    \n",
    "    for en, ch_gr, ch_pd in records:\n",
    "        # caculate BLEU by nltk\n",
    "        bleu = nltk.translate.bleu_score.sentence_bleu([ch_gr], ch_pd)\n",
    "        bleus.append(bleu)\n",
    "    \n",
    "    lst = [i for i in range(len(records)) if bleus[i]<=upper_bound]\n",
    "    lst = sorted(lst, key=lambda i: bleus[i], reverse=True) # sort by BLEU score\n",
    "    \n",
    "    return [records[lst[i]] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input\n",
      "Ground truth\n",
      "Decoder output\n",
      "\n",
      "i have to get my teeth cleaned this week\n",
      "oh that is nice\n",
      "oh that is nice\n",
      "\n",
      "i do not see anything maybe just a little\n",
      "holy shit i am a freak\n",
      "holy shit i am a freak\n",
      "\n",
      "bad luck to see death in the snow\n",
      "but what happened to the little deer\n",
      "but what happened to the little deer\n",
      "\n",
      "i hate tears\n",
      "is mary in trouble\n",
      "is mary in trouble\n",
      "\n",
      "yeah i guess it is all right\n",
      "are you all right\n",
      "are you all right\n",
      "\n",
      "mommy please help me\n",
      "do not be afraid\n",
      "do not be afraid\n",
      "\n",
      "just not always in the sexual sense\n",
      "you are hurting me\n",
      "you are hurting me\n",
      "\n",
      "do not you ever have any problems\n",
      "i have one now\n",
      "i have one now\n",
      "\n",
      "oh mrs crawford do not believe in them\n",
      "well maybe she better start\n",
      "well maybe she better start\n",
      "\n",
      "you tell maggie\n",
      "no you tell her\n",
      "no you tell her\n",
      "\n",
      "actually he sounded nice\n",
      "oh oh really now we are getting down to it\n",
      "oh oh really now we are getting down to it\n",
      "\n",
      "you tell maggie\n",
      "no you tell her\n",
      "no you tell her\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "\n",
    "records = []\n",
    "\n",
    "for i in range(10):\n",
    "    i = rd.randint(0, batch_num-1) # random pick one to translate\n",
    "    \n",
    "    x, y, g, w = batch.get(i)\n",
    "    out = model.output(x, y)\n",
    "    pd = model.predict(x, ans_vocab['<BEG>'])\n",
    "\n",
    "    for j in range(10):\n",
    "        j = rd.randint(0, BATCH_SIZE-1)\n",
    "        \n",
    "        en = [ques_rev[x[i][j]] for i in range(en_max_len)]\n",
    "        en = en[:en.index('<PAD>')]\n",
    "        ch_gr = [ans_rev[g[i][j]] for i in range(ch_max_len)]\n",
    "        if '<END>' in ch_gr:\n",
    "            ch_gr = ch_gr[:ch_gr.index('<END>')]\n",
    "        ch_pd = [ques_rev[np.argmax(pd[i][j, :])] for i in range(ch_max_len)]\n",
    "        if '<END>' in ch_pd:\n",
    "            ch_pd = ch_pd[:ch_pd.index('<END>')]\n",
    "        \n",
    "        records.append([en, ch_gr, ch_pd])\n",
    "\n",
    "n = 12 # how many result we show\n",
    "rec_cherry = cherry_pick(records, n)\n",
    "\n",
    "print(\"Encoder input\")\n",
    "print(\"Ground truth\")\n",
    "print(\"Decoder output\")\n",
    "print()\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(3):\n",
    "        print(' '.join(rec_cherry[i][j]))\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Let your model do the conversations below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仿照前面的處理，再丟進model 做 predict，不知道為甚麼shape會與batch size一樣，所以在此把要回答的問題，重複組成一個128的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_input = [\"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\", \"you look great\", \"good night\",\n",
    "              \"hello\", \"how are you\", \"where are you going\"]\n",
    "\n",
    "for i in range(len(ques_input)):\n",
    "    ques_input[i] = '<BEG> '+ques_input[i]+' <END>'\n",
    "#print(ques_input)\n",
    "ques_input_toInt = []\n",
    "for question in ques_input:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    ques_input_toInt.append(ints)\n",
    "#print(ques_input_toInt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "how are you <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "where are you going <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "you look great <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "good night <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "ques_xs = [np.zeros(len(ques_input_toInt), dtype=np.int32) for _ in range(en_max_len)]\n",
    "count = 0\n",
    "for i in range(len(ques_input)):\n",
    "    for j in range(len(ques_input_toInt[i])-2):\n",
    "        ques_xs[j][i] = ques_input_toInt[i][j+1]\n",
    "    for j in range(j + 1, en_max_len):\n",
    "        ques_xs[j][i] = ques_vocab['<PAD>']\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(' '.join([ques_rev[ques_xs[j][i]] for j in range(en_max_len)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = model.predict(ques_xs, ans_vocab['<BEG>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : ['hello']\n",
      "B : ['hi', 'late', 'i', 'am', 'late', 'for', 'christmas', 'this', 'morning']\n",
      "\n",
      "A : ['how', 'are', 'you']\n",
      "B : ['another', 'young', 'man']\n",
      "\n",
      "A : ['where', 'are', 'you', 'going']\n",
      "B : ['to', 'complete', 'my', 'cure', 'i', 'am', 'just', 'hiding']\n",
      "\n",
      "A : ['you', 'look', 'great']\n",
      "B : ['thanks', 'i', 'just', 'get', 'into', 'this', 'calling', 'jake']\n",
      "\n",
      "A : ['good', 'night']\n",
      "B : ['good', 'night']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(5):\n",
    "    qes = [ques_rev[ques_xs[i][j]] for i in range(max_line_length)]\n",
    "    ans = [ques_rev[np.argmax(pd[i][j, :])] for i in range(max_line_length)]\n",
    "    ques_final = []\n",
    "    ans_final = []\n",
    "    for item in qes:\n",
    "        if not item == '<PAD>':                      \n",
    "            ques_final.append(item)\n",
    "    for item in ans:\n",
    "        if not item == '<END>':                      \n",
    "            ans_final.append(item)\n",
    "    print(\"A : {}\" .format(ques_final))\n",
    "    print(\"B : {}\" .format(ans_final))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第 1 個對話場景 : B 今天早上要去 A (Late) 的家開聖誕轟趴，可是萬萬沒想到他媽媽需要出門買菜，叫他照顧年幼的弟弟，結果快到中午才趕到Late的家，B進了A家門後，開始了這段對話。\n",
    "\n",
    "第 2 個對話場景 : B可能是老年癡呆，A是醫生看他最近回答有沒有進步........QQ 掰不下去。\n",
    "\n",
    "第 3 個對話場景 : A和B在醫院外面遇到，A就問\"where are you going\"，B就說\"to complete my cure i am just hiding \"。因為A和B是麻吉，B就把他隱瞞已久的病情跟A講。\n",
    "\n",
    "第 4 個對話場景 : A在路上看到B，B頂著一頭酷炫的頭髮，A就說\"you look great\"，B就說 \"thanks i just get into this calling jake\" 謝謝我才剛進去這間理髮店叫jake。\n",
    "\n",
    "第 5 個對話場景 : A和B是正在講電話的情侶，最後準備要睡時，他們就互道晚安，掛了電話。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
