{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Member：\n",
    "\n",
    "107062514 賴鵬仁\n",
    "\n",
    "107062616 傅品捷\n",
    "\n",
    "107065513 姚定嘉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook uses TensorFlow version 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "\n",
    "print(\"This notebook uses TensorFlow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 26900 vocabularies\n"
     ]
    }
   ],
   "source": [
    "vocab = cPickle.load(open('dataset/text/vocab.pkl', 'rb'))\n",
    "print('total {} vocabularies'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vocabulary occurances...\n",
      "3153 words appear >= 50 times\n"
     ]
    }
   ],
   "source": [
    "def count_vocab_occurance(vocab, df):\n",
    "    voc_cnt = {v: 0 for v in vocab}\n",
    "    for img_id, row in df.iterrows():\n",
    "        for w in row['caption'].split(' '):\n",
    "            voc_cnt[w] += 1\n",
    "    return voc_cnt\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(os.path.join('dataset', 'train.csv'))\n",
    "\n",
    "print('count vocabulary occurances...')\n",
    "voc_cnt = count_vocab_occurance(vocab, df_train)\n",
    "\n",
    "# remove words appear < 50 times\n",
    "thrhd = 50\n",
    "x = np.array(list(voc_cnt.values()))\n",
    "print('{} words appear >= 50 times'.format(np.sum(x[(-x).argsort()] >= thrhd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voc_mapping(voc_cnt, thrhd):\n",
    "    \"\"\"\n",
    "    enc_map: voc --encode--> id\n",
    "    dec_map: id --decode--> voc\n",
    "    \"\"\"\n",
    "\n",
    "    def add(enc_map, dec_map, voc):\n",
    "        enc_map[voc] = len(dec_map)\n",
    "        dec_map[len(dec_map)] = voc\n",
    "        return enc_map, dec_map\n",
    "\n",
    "    # add <ST>, <ED>, <RARE>\n",
    "    enc_map, dec_map = {}, {}\n",
    "    for voc in ['<ST>', '<ED>', '<RARE>']:\n",
    "        enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    for voc, cnt in voc_cnt.items():\n",
    "        if cnt < thrhd:  # rare words => <RARE>\n",
    "            enc_map[voc] = enc_map['<RARE>']\n",
    "        else:\n",
    "            enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    return enc_map, dec_map\n",
    "\n",
    "\n",
    "enc_map, dec_map = build_voc_mapping(voc_cnt, thrhd)\n",
    "# save enc/decoding map to disk\n",
    "cPickle.dump(enc_map, open('dataset/text/enc_map.pkl', 'wb'))\n",
    "cPickle.dump(dec_map, open('dataset/text/dec_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transform captions into sequences of IDs]...\n"
     ]
    }
   ],
   "source": [
    "def caption_to_ids(enc_map, df):\n",
    "    img_ids, caps = [], []\n",
    "    for idx, row in df.iterrows():\n",
    "        icap = [enc_map[x] for x in row['caption'].split(' ')]\n",
    "        icap.insert(0, enc_map['<ST>'])\n",
    "        icap.append(enc_map['<ED>'])\n",
    "        img_ids.append(row['img_id'])\n",
    "        caps.append(icap)\n",
    "    return pd.DataFrame({\n",
    "              'img_id': img_ids,\n",
    "              'caption': caps\n",
    "            }).set_index(['img_id'])\n",
    "\n",
    "\n",
    "enc_map = cPickle.load(open('dataset/text/enc_map.pkl', 'rb'))\n",
    "print('[transform captions into sequences of IDs]...')\n",
    "df_proc = caption_to_ids(enc_map, df_train)\n",
    "df_proc.to_csv('dataset/text/train_enc_cap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding the encoded captions back...\n",
      "\n",
      "0: <ST> a group of three women sitting at a table sharing a cup of tea <ED>\n",
      "1: <ST> three women wearing hats at a table together <ED>\n",
      "2: <ST> three women with hats at a table having a tea party <ED>\n",
      "3: <ST> several woman dressed up with fancy hats at a tea party <ED>\n",
      "4: <ST> three women wearing large hats at a fancy tea event <ED>\n",
      "5: <ST> a twin door refrigerator in a kitchen next to cabinets <ED>\n",
      "6: <ST> a black refrigerator freezer sitting inside of a kitchen <ED>\n",
      "7: <ST> black refrigerator in messy kitchen of residential home <ED>\n"
     ]
    }
   ],
   "source": [
    "df_cap = pd.read_csv(\n",
    "    'dataset/text/train_enc_cap.csv')  # a dataframe - 'img_id', 'cpation'\n",
    "enc_map = cPickle.load(\n",
    "    open('dataset/text/enc_map.pkl', 'rb'))  # token => id\n",
    "dec_map = cPickle.load(\n",
    "    open('dataset/text/dec_map.pkl', 'rb'))  # id => token\n",
    "vocab_size = len(dec_map)\n",
    "\n",
    "\n",
    "def decode(dec_map, ids):\n",
    "    \"\"\"decode IDs back to origin caption string\"\"\"\n",
    "    return ' '.join([dec_map[x] for x in ids])\n",
    "\n",
    "\n",
    "print('decoding the encoded captions back...\\n')\n",
    "for idx, row in df_cap.iloc[:8].iterrows():\n",
    "    print('{}: {}'.format(idx, decode(dec_map, eval(row['caption']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list = []\n",
    "\n",
    "for row in df_cap['caption']:\n",
    "    len_list.append(len(eval(row)))\n",
    "\n",
    "len_list = pd.DataFrame(len_list, columns=['cap_len'])\n",
    "\n",
    "df_cap_sorted = pd.concat([df_cap, len_list], axis=1, join='inner')\n",
    "df_cap_sorted = df_cap_sorted.sort_values(by = 'cap_len', ascending=True).drop('cap_len',axis =1)\n",
    "df_cap_sorted.to_csv('dataset/text/train_enc_cap_sorted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def create_tfrecords(df_cap, img_df, filename, num_files=5):\n",
    "\n",
    "    def _float_feature(value):\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "    num_records_per_file = df_cap.shape[0] // num_files\n",
    "\n",
    "    total_count = 0\n",
    "\n",
    "    print (\"creat training dataset......\")\n",
    "\n",
    "    for i in range(num_files):\n",
    "        count = 0\n",
    "        writer = tf.python_io.TFRecordWriter(filename + '-' + str(i+1) + '.tfrecord')\n",
    "\n",
    "        start = i * num_records_per_file\n",
    "        end = (i+1) * num_records_per_file if i != num_files-1 else df_cap.shape[0]\n",
    "\n",
    "        for idx, row in df_cap.iloc[start: end].iterrows():\n",
    "            caption = eval(row['caption'])\n",
    "\n",
    "            for _, inner_row in img_df[img_df['img_id'] == row['img_id']].iterrows():\n",
    "                img_representation = inner_row['img']\n",
    "\n",
    "                example = tf.train.Example(\n",
    "                        features=tf.train.Features(feature={\n",
    "                                'img': _float_feature(img_representation),\n",
    "                                'caption': _int64_feature(caption)\n",
    "                            })\n",
    "                        )\n",
    "\n",
    "                count += 1\n",
    "                writer.write(example.SerializeToString())\n",
    "\n",
    "        print (\"create {}-{}.tfrecord -- contains {} records\".format(filename, str(i+1), count))\n",
    "        total_count += count\n",
    "        writer.close()\n",
    "\n",
    "    print (\"Total records: {}\".format(total_count))\n",
    "\n",
    "\n",
    "df_cap = pd.read_csv('dataset/text/train_enc_cap_sorted.csv')\n",
    "img_train = cPickle.load(open('dataset/train_img256.pkl', 'rb'))\n",
    "\n",
    "img_train_df = pd.DataFrame(list(img_train.items()), columns=['img_id', 'img'])\n",
    "\n",
    "create_tfrecords(df_cap, img_train_df, 'dataset/tfrecord/train', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curricular Learning\n",
    "在此我們按照notebook給的提示加上Curricular Learning，將字數短的視為簡單的task，長的視為難的task，讓model比較好train。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images for training: 102739\n"
     ]
    }
   ],
   "source": [
    "img_train = cPickle.load(open('dataset/train_img256.pkl', 'rb'))\n",
    "# transform img_dict to dataframe\n",
    "img_train_df = pd.DataFrame(list(img_train.items()), columns=['img_id', 'img'])\n",
    "print('Images for training: {}'.format(img_train_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records in all training file: 513969\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "training_filenames = glob.glob('dataset/tfrecords/train-*')\n",
    "\n",
    "# get the number of records in training files\n",
    "def get_num_records(files):\n",
    "    count = 0\n",
    "    for fn in files:\n",
    "        for record in tf.python_io.tf_record_iterator(fn):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "num_train_records = get_num_records(training_filenames)\n",
    "print('Number of training records in all training file: {}'.format(\n",
    "    num_train_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_parser(record):\n",
    "    ''' parse record from .tfrecords file and create training record\n",
    "\n",
    "    :args \n",
    "      record - each record extracted from .tfrecords\n",
    "    :return\n",
    "      a dictionary contains {\n",
    "          'img': image array extracted from vgg16 (256-dim),\n",
    "          'input_seq': a list of word id\n",
    "                    which describes input caption sequence (Tensor),\n",
    "          'output_seq': a list of word id\n",
    "                    which describes output caption sequence (Tensor),\n",
    "          'mask': a list of one which describe\n",
    "                    the length of input caption sequence (Tensor)\n",
    "      }\n",
    "    '''\n",
    "\n",
    "    keys_to_features = {\n",
    "      \"img\": tf.FixedLenFeature([256], dtype=tf.float32),\n",
    "      \"caption\": tf.VarLenFeature(dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # features contains - 'img', 'caption'\n",
    "    features = tf.parse_single_example(record, features=keys_to_features)\n",
    "\n",
    "    img = features['img']\n",
    "    caption = features['caption'].values\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    # create input and output sequence for each training example\n",
    "    # e.g. caption :   [0 2 5 7 9 1]\n",
    "    #      input_seq:  [0 2 5 7 9]\n",
    "    #      output_seq: [2 5 7 9 1]\n",
    "    #      mask:       [1 1 1 1 1]\n",
    "    caption_len = tf.shape(caption)[0]\n",
    "    input_len = tf.expand_dims(tf.subtract(caption_len, 1), 0)\n",
    "\n",
    "    input_seq = tf.slice(caption, [0], input_len)\n",
    "    output_seq = tf.slice(caption, [1], input_len)\n",
    "    mask = tf.ones(input_len, dtype=tf.int32)\n",
    "\n",
    "    records = {\n",
    "      'img': img,\n",
    "      'input_seq': input_seq,\n",
    "      'output_seq': output_seq,\n",
    "      'mask': mask\n",
    "    }\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_iterator(filenames, batch_size, record_parser):\n",
    "    ''' create iterator to eat tfrecord dataset \n",
    "\n",
    "    :args\n",
    "        filenames     - a list of filenames (string)\n",
    "        batch_size    - batch size (positive int)\n",
    "        record_parser - a parser that read tfrecord\n",
    "                        and create example record (function)\n",
    "\n",
    "    :return \n",
    "        iterator      - an Iterator providing a way\n",
    "                        to extract elements from the created dataset.\n",
    "        output_types  - the output types of the created dataset.\n",
    "        output_shapes - the output shapes of the created dataset.\n",
    "    '''\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(record_parser, num_parallel_calls=16)\n",
    "\n",
    "    # padded into equal length in each batch\n",
    "    dataset = dataset.padded_batch(\n",
    "      batch_size=batch_size,\n",
    "      padded_shapes={\n",
    "          'img': [None],\n",
    "          'input_seq': [None],\n",
    "          'output_seq': [None],\n",
    "          'mask': [None]\n",
    "      },\n",
    "      padding_values={\n",
    "          'img': 1.0,       # needless, for completeness\n",
    "          'input_seq': 1,   # padding input sequence in this batch\n",
    "          'output_seq': 1,  # padding output sequence in this batch\n",
    "          'mask': 0         # padding 0 means no words in this position\n",
    "      })  \n",
    "\n",
    "    dataset = dataset.repeat()             # repeat dataset infinitely\n",
    "    dataset = dataset.shuffle(10*batch_size)  # shuffle the dataset\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為使用Curricular Learning，model吃到的data都是先從字數少的開始，再慢慢到字數多的data，後來我們認為這樣model可能會學習到\"先短再長\"的order，導致generalizability不好，所以我們在training時，在dataset.shuffle()的buffer size先設為1 * batch_size，然後漸漸增加buffer size，使data可以洗更亂，避免上述問題，實驗結果發現此方法是可行的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(object):\n",
    "    ''' simple image caption model '''\n",
    "\n",
    "    def __init__(self, hparams, mode):\n",
    "        self.hps = hparams\n",
    "        self.mode = mode\n",
    "        self.ckpt_dir = './model/'\n",
    "    def _build_inputs(self):\n",
    "        \"\"\" construct the inputs for model \"\"\"\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.filenames = tf.placeholder(tf.string,\n",
    "                                        shape=[None], name='filenames')\n",
    "            self.training_iterator, types, shapes = tfrecord_iterator(\n",
    "            self.filenames, self.hps.batch_size, training_parser)\n",
    "                \n",
    "            self.handle = tf.placeholder(tf.string, shape=[], name='handle')\n",
    "            iterator = tf.data.Iterator.from_string_handle(self.handle,\n",
    "                                                       types, shapes)\n",
    "            records = iterator.get_next()\n",
    "            image_embed = records['img']\n",
    "            image_embed.set_shape([None, self.hps.image_embedding_size])\n",
    "            input_seq = records['input_seq']\n",
    "            target_seq = records['output_seq']\n",
    "            input_mask = records['mask']\n",
    "            \n",
    "        else:\n",
    "            self.image_embed_feed = tf.placeholder(dtype=tf.float32, shape=[None, self.hps.image_embedding_size],\n",
    "                                              name=\"image_embed_feed\")\n",
    "        \n",
    "            self.input_seq_feed = tf.placeholder(dtype=tf.int32, shape=[None],\n",
    "                                              name=\"input_seq_feed\")\n",
    "            input_seq = tf.expand_dims(self.input_seq_feed, axis=1)\n",
    "            image_embed = self.image_embed_feed\n",
    "            #input_seq = self.input_seq_feed\n",
    "            target_seq = None\n",
    "            input_mask = None\n",
    "            \n",
    "        self.image_embed = image_embed # (batch_size, img_dim)\n",
    "        self.input_seq = input_seq # (batch_size, seqlen)\n",
    "        self.target_seq = target_seq # (batch_size, seqlen)\n",
    "        self.input_mask = input_mask # (batch_size, seqlen)\n",
    "                \n",
    "        # convert sequence of index to sequence of embedding\n",
    "    def _build_seq_embeddings(self):\n",
    "        with tf.variable_scope('seq_embedding') as scope, tf.device('/cpu:0'):\n",
    "            embedding_matrix = tf.get_variable(\n",
    "                  name='embedding_matrix',\n",
    "                  shape=[self.hps.vocab_size, self.hps.word_embedding_size],\n",
    "                  initializer=tf.random_uniform_initializer(minval=-1, maxval=1))\n",
    "            # [batch_size, padded_length, embedding_size]\n",
    "            seq_embeddings = tf.nn.embedding_lookup(embedding_matrix, self.input_seq)\n",
    "            scope.reuse_variables()\n",
    "        self.seq_embeddings = seq_embeddings\n",
    "            \n",
    "    def _build_model(self):\n",
    "        \"\"\" Build your image caption model \"\"\"\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.hps.rnn_units, state_is_tuple=True)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell,\n",
    "            input_keep_prob=self.hps.drop_keep_prob,\n",
    "            output_keep_prob=self.hps.drop_keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"lstm\", initializer=tf.random_uniform_initializer(\n",
    "                        minval=-1, maxval=1)) as lstm_scope:\n",
    "            zero_state = lstm_cell.zero_state(batch_size=tf.shape(self.image_embed)[0], dtype=tf.float32)###\n",
    "            _, initial_state = lstm_cell(self.image_embed, zero_state)\n",
    "                        \n",
    "            lstm_scope.reuse_variables()\n",
    "            \n",
    "            if self.mode == 'inference':\n",
    "                # In inference mode, use concatenated states for convenient feeding and\n",
    "                # fetching.\n",
    "                self.concat_init_state = tf.concat(axis=1, values=initial_state, name=\"initial_state\")\n",
    "\n",
    "                # Placeholder for feeding a batch of concatenated states.\n",
    "                self.state_feed = tf.placeholder(dtype=tf.float32,\n",
    "                                            shape=[None, sum(lstm_cell.state_size)],\n",
    "                                            name=\"state_feed\")\n",
    "                state_tuple = tf.split(value=self.state_feed, num_or_size_splits=2, axis=1)\n",
    "\n",
    "                # Run a single LSTM step.\n",
    "                lstm_outputs, state_tuple = lstm_cell(\n",
    "                    inputs=tf.squeeze(self.seq_embeddings, axis=[1]),\n",
    "                    state=state_tuple)\n",
    "\n",
    "                # Concatentate the resulting state.\n",
    "                self.concat_output_state = tf.concat(axis=1, values=state_tuple, name=\"state\")\n",
    "                \n",
    "            else:\n",
    "                sequence_length = tf.reduce_sum(self.input_mask, 1)\n",
    "                lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                                    inputs=self.seq_embeddings,\n",
    "                                                    sequence_length=sequence_length,\n",
    "                                                    initial_state=initial_state,\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    scope=lstm_scope)\n",
    "        \n",
    "        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n",
    "        \n",
    "        with tf.variable_scope(\"logits\") as logits_scope:\n",
    "            logits = tf.contrib.layers.fully_connected(\n",
    "            inputs=lstm_outputs,\n",
    "            num_outputs=self.hps.vocab_size,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=tf.random_uniform_initializer(minval=-1, maxval=1),\n",
    "            scope=logits_scope)\n",
    "            \n",
    "        if self.mode == 'inference':\n",
    "            self.softmax_output = tf.nn.softmax(logits, name=\"softmax\")\n",
    "            self.prediction = tf.argmax(self.softmax_output, axis=1, name='prediction')\n",
    "        else:\n",
    "            targets = tf.reshape(self.target_seq, [-1])\n",
    "            weights = tf.cast(tf.reshape(self.input_mask, [-1]), tf.float32)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,logits=logits)\n",
    "            self.batch_loss = tf.div(tf.reduce_sum(tf.multiply(losses, weights)),\n",
    "                          tf.reduce_sum(weights),\n",
    "                          name=\"batch_loss\")\n",
    "            \n",
    "            #tf.losses.add_loss(self.batch_loss)\n",
    "            #total_loss = tf.losses.get_total_loss()\n",
    "            self.total_loss = self.batch_loss\n",
    "            self.target_cross_entropy_losses = losses  # Used in evaluation.\n",
    "            self.target_cross_entropy_loss_weights = weights  # Used in evaluation.\n",
    "            \n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.hps.lr)\n",
    "            grads_and_vars = optimizer.compute_gradients(self.total_loss,\n",
    "                                             tf.trainable_variables())\n",
    "            clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], 1.0), gv[1])\n",
    "                          for gv in grads_and_vars]\n",
    "            self.train_op = optimizer.apply_gradients(clipped_grads_and_vars)\n",
    "            \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\" call this function to build the inputs and model \"\"\"\n",
    "        self._build_inputs()\n",
    "        self._build_seq_embeddings()\n",
    "        self._build_model()\n",
    "\n",
    "        \n",
    "    def train(self, sess, training_filenames, num_train_records):\n",
    "        \"\"\" write a training function for your model \"\"\"\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        self.mode = \"train\"\n",
    "        ckpt = tf.train.get_checkpoint_state(self.hps.ckpt_dir)\n",
    "        ckpt_path = self.ckpt_dir\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            # if checkpoint exists\n",
    "            #saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            #####saver.restore(sess, ckpt_path)\n",
    "                \n",
    "            self.saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))\n",
    "                \n",
    "            # assume the name of checkpoint is like '.../model.ckpt-1000'\n",
    "            #####gs = int(ckpt_path.split('/')[-1].split('-')[-1])\n",
    "            #gs = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])\n",
    "            #sess.run(tf.assign(global_step, gs))\n",
    "        else:\n",
    "            # no checkpoint\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "    \n",
    "        ##ˇsess.run(tf.global_variables_initializer())\n",
    "        batch_num = num_train_records//self.hps.batch_size\n",
    "        \n",
    "        training_handle = sess.run(self.training_iterator.string_handle())\n",
    "        sess.run(self.training_iterator.initializer,feed_dict={self.filenames: training_filenames})\n",
    "        \n",
    "        for e in range(self.hps.training_epochs):\n",
    "            train_loss = 0\n",
    "            for b in range(batch_num):\n",
    "                loss, _ = sess.run([self.total_loss, self.train_op], feed_dict={self.handle:training_handle})\n",
    "                train_loss += loss\n",
    "\n",
    "            if e % 10 == 0:\n",
    "                self.saver.save(sess, 'model/model_%d.ckpt'%(e))\n",
    "            print(\"epoch %d loss: %f\" % (e, train_loss))\n",
    "            \n",
    "        pass\n",
    "    \n",
    "    def predict(self, sess, img_vec, dec_map):\n",
    "        \"\"\" generate the caption given an image \"\"\"\n",
    "        self.mode = \"inference\"\n",
    "        caption_id = []\n",
    "        \n",
    "        '''stack elements to match the batch size\n",
    "        \n",
    "        def get_duplicate_form(element): \n",
    "            elements = []\n",
    "            for i in range(self.hps.batch_size):\n",
    "                elements.append(element)\n",
    "            elements = np.array(elements)\n",
    "            return elements\n",
    "        '''\n",
    "        #img_vecs = get_duplicate_form(img_vec)\n",
    "        st = [enc_map['<ST>']]\n",
    "        \n",
    "        #st = get_duplicate_form([enc_map['<ST>']])\n",
    "        \n",
    "        concat_init_state = sess.run(self.concat_init_state, feed_dict={self.image_embed_feed:img_vec})\n",
    "        \n",
    "        next_input,concat_output_state = sess.run([self.prediction,self.concat_output_state],\n",
    "                                  feed_dict={self.input_seq_feed:st, self.state_feed:concat_init_state})\n",
    "        \n",
    "        #softmax_output = sess.run(self.softmax_output, feed_dict={self.input_seq_feed:st, self.state_feed:concat_init_state})\n",
    "        \n",
    "        #pre_state = concat_output_state\n",
    "        #next_input = np.argmax(softmax_output[0])\n",
    "        caption_id.append(int(next_input))\n",
    "        \n",
    "        for i in range(self.hps.max_caption_len - 1):\n",
    "            # st= get_duplicate_form([next_input])\n",
    "            #st = next_input\n",
    "            next_input,concat_output_state = sess.run([self.prediction,self.concat_output_state],\n",
    "                                          feed_dict={self.input_seq_feed:next_input, self.state_feed:concat_output_state})\n",
    "\n",
    "            #softmax_output = sess.run(self.softmax_output, feed_dict={self.image_embed_feed:img_vecs,\n",
    "            #                                         self.input_seq_feed:st, self.state_feed:pre_state}) \n",
    "            \n",
    "            #pre_state = concat_output_state\n",
    "            #next_input = np.argmax(softmax_output[0])\n",
    "            caption_id.append(int(next_input))\n",
    "            #if(dec_map[next_input] == '<ED>'):\n",
    "            #break\n",
    "        #caption.append(dec_map[next_input])\n",
    "        caption = [\n",
    "                dec_map[x]\n",
    "                for x in caption_id[:None\n",
    "                                    if enc_map['<ED>'] not in caption_id else caption_id.index(enc_map['<ED>'])]    \n",
    "        ]\n",
    "        return ' '.join(caption)    \n",
    "    \n",
    "    def restore(self,sess, e):\n",
    "        self.saver.restore(sess, 'model/model_%d.ckpt'%(e))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "show and tell model。我們按照助教給的Hint，加入Gradient-Clipping，防止梯度爆炸的問題，正如老師上課所說LSTM + Gradient-Clipping是個好的組合，這裡我們沒有使用Attention machanism，inference也沒有使用Beam search，只用最簡單的greedy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "      vocab_size=vocab_size,\n",
    "      batch_size=64,\n",
    "      rnn_units=256,\n",
    "      image_embedding_size=256,\n",
    "      word_embedding_size=256,\n",
    "      drop_keep_prob=0.7,\n",
    "      lr=1e-5,\n",
    "      training_epochs=101,\n",
    "      max_caption_len=15,\n",
    "      ckpt_dir='model/')\n",
    "    return hparams\n",
    "# get hperparameters\n",
    "hparams = get_hparams()\n",
    "# create model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡我們把rnn_unit從100改成256，讓LSTM的capacity增加，希望可以使performance更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_100.ckpt\n",
      "epoch 0 loss: 21404.259413\n",
      "epoch 1 loss: 20927.326910\n",
      "epoch 2 loss: 20608.918218\n",
      "epoch 3 loss: 20512.934898\n",
      "epoch 4 loss: 20456.215910\n",
      "epoch 5 loss: 20405.894823\n",
      "epoch 6 loss: 20394.569171\n",
      "epoch 7 loss: 20382.877614\n",
      "epoch 8 loss: 20361.770439\n",
      "epoch 9 loss: 20365.393644\n",
      "epoch 10 loss: 20361.717659\n",
      "epoch 11 loss: 20366.450012\n",
      "epoch 12 loss: 20364.004521\n",
      "epoch 13 loss: 20340.287861\n",
      "epoch 14 loss: 20345.051438\n",
      "epoch 15 loss: 20343.848436\n",
      "epoch 16 loss: 20339.081120\n",
      "epoch 17 loss: 20339.734980\n",
      "epoch 18 loss: 20339.048450\n",
      "epoch 19 loss: 20331.944392\n",
      "epoch 20 loss: 20333.622939\n",
      "epoch 21 loss: 20313.178636\n",
      "epoch 22 loss: 20344.359597\n",
      "epoch 23 loss: 20320.337561\n",
      "epoch 24 loss: 20316.496462\n",
      "epoch 25 loss: 20313.385135\n",
      "epoch 26 loss: 20310.332893\n",
      "epoch 27 loss: 20311.916967\n",
      "epoch 28 loss: 20321.132531\n",
      "epoch 29 loss: 20311.296660\n",
      "epoch 30 loss: 20301.419603\n",
      "epoch 31 loss: 20302.070383\n",
      "epoch 32 loss: 20302.818284\n",
      "epoch 33 loss: 20298.646810\n",
      "epoch 34 loss: 20313.107387\n",
      "epoch 35 loss: 20294.453860\n",
      "epoch 36 loss: 20295.455621\n",
      "epoch 37 loss: 20290.489663\n",
      "epoch 38 loss: 20288.350313\n",
      "epoch 39 loss: 20293.558193\n",
      "epoch 40 loss: 20291.490155\n",
      "epoch 41 loss: 20278.999078\n",
      "epoch 42 loss: 20293.741818\n",
      "epoch 43 loss: 20295.780137\n",
      "epoch 44 loss: 20276.869200\n",
      "epoch 45 loss: 20290.387314\n",
      "epoch 46 loss: 20269.306572\n",
      "epoch 47 loss: 20274.781191\n",
      "epoch 48 loss: 20270.309277\n",
      "epoch 49 loss: 20276.974947\n",
      "epoch 50 loss: 20275.067642\n",
      "epoch 51 loss: 20268.348261\n",
      "epoch 52 loss: 20273.834373\n",
      "epoch 53 loss: 20281.154041\n",
      "epoch 54 loss: 20261.605624\n",
      "epoch 55 loss: 20270.863794\n",
      "epoch 56 loss: 20266.953720\n",
      "epoch 57 loss: 20269.897712\n",
      "epoch 58 loss: 20253.674426\n",
      "epoch 59 loss: 20267.235137\n",
      "epoch 60 loss: 20275.547295\n",
      "epoch 61 loss: 20264.100076\n",
      "epoch 62 loss: 20264.460254\n",
      "epoch 63 loss: 20261.735204\n",
      "epoch 64 loss: 20257.807729\n",
      "epoch 65 loss: 20261.280139\n",
      "epoch 66 loss: 20245.049744\n",
      "epoch 67 loss: 20262.600532\n",
      "epoch 68 loss: 20255.193527\n",
      "epoch 69 loss: 20256.616509\n",
      "epoch 70 loss: 20250.796657\n",
      "epoch 71 loss: 20256.648820\n",
      "epoch 72 loss: 20255.193844\n",
      "epoch 73 loss: 20265.999906\n",
      "epoch 74 loss: 20237.259326\n",
      "epoch 75 loss: 20246.635610\n",
      "epoch 76 loss: 20249.156074\n",
      "epoch 77 loss: 20248.685563\n",
      "epoch 78 loss: 20253.901598\n",
      "epoch 79 loss: 20258.355666\n",
      "epoch 80 loss: 20242.129601\n",
      "epoch 81 loss: 20233.880871\n",
      "epoch 82 loss: 20249.029844\n",
      "epoch 83 loss: 20240.590498\n",
      "epoch 84 loss: 20241.359689\n",
      "epoch 85 loss: 20243.792092\n",
      "epoch 86 loss: 20243.880768\n",
      "epoch 87 loss: 20243.080946\n",
      "epoch 88 loss: 20239.062998\n",
      "epoch 89 loss: 20243.168465\n",
      "epoch 90 loss: 20247.054814\n",
      "epoch 91 loss: 20224.183262\n",
      "epoch 92 loss: 20243.292222\n",
      "epoch 93 loss: 20232.901185\n",
      "epoch 94 loss: 20234.221963\n",
      "epoch 95 loss: 20238.007678\n",
      "epoch 96 loss: 20228.576307\n",
      "epoch 97 loss: 20234.447484\n",
      "epoch 98 loss: 20241.705326\n",
      "epoch 99 loss: 20222.681836\n",
      "epoch 100 loss: 20237.126007\n"
     ]
    }
   ],
   "source": [
    "model = ImageCaptionModel(hparams, 'train')\n",
    "model.build()\n",
    "# start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "model.train(sess, training_filenames, num_train_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "在此我們用我們上述提的假設(shuffle更亂)，陸續train了大約400個epochs，中間都有停掉再restore model，更改dataset.shuffle()的buffer size，再繼續train。我們到最後也有嘗試把Adam改成SGD，因為聽到上次第一名的分享，做了這樣的嘗試。到後來loss一直降不下去，但是產生出來的score卻逐漸進步，我們認為training loss只是個參考的依據，並非最後決定結果好壞的絕對標準。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_90.ckpt\n",
      "233954.jpg\n",
      "a   l i v i n g   r o o m   w i t h   a   c o u c h   a n d   a   t e l e v i s i o n \n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "model = ImageCaptionModel(hparams, \"inference\")\n",
    "model.build()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, 'model/model_90.ckpt')\n",
    "\n",
    "assert (hparams.rnn_units == hparams.image_embedding_size)\n",
    "testimg = img_train_df.iloc[9]['img']\n",
    "testimg = np.expand_dims(testimg, axis=0)\n",
    "print(img_train_df.iloc[9]['img_id'])\n",
    "\n",
    "\n",
    "c_list = model.predict(sess, testimg, dec_map)\n",
    "caption = ''\n",
    "for word in c_list:\n",
    "    caption += word + ' '\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from pretrained.cnn import PretrainedCNN\n",
    "import imageio\n",
    "import skimage.transform\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "def demo(img_path, cnn_mdl, U, dec_map, hparams, max_len=15):\n",
    "    \"\"\"\n",
    "    displays the caption generated for the image\n",
    "    -------------------------------\n",
    "    img_path: image to be captioned\n",
    "    cnn_mdl: path of the image feature extractor\n",
    "    U: transform matrix to perform PCA\n",
    "    dec_map: mapping of vocabulary ID => token string\n",
    "    hparams: hyperparams for model\n",
    "    \"\"\"\n",
    "\n",
    "    def process_image(img, crop=True, submean=True):\n",
    "        \"\"\"\n",
    "        implements the image preprocess required by VGG-16\n",
    "        -------------------------------\n",
    "        resize image to 224 x 224\n",
    "        crop: do center-crop [skipped by default]\n",
    "        submean: substracts mean image of ImageNet [skipped by default]\n",
    "        \"\"\"\n",
    "        MEAN = np.array([103.939, 116.779, 123.68]).astype(np.float32) # BGR\n",
    "        # center crop\n",
    "        short_edge = min(img.shape[:2])\n",
    "        yy = int((img.shape[0] - short_edge) / 2)\n",
    "        xx = int((img.shape[1] - short_edge) / 2)\n",
    "        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "        img = scipy.misc.imresize(crop_img, [224, 224, 3])\n",
    "        img = img.reshape((224,224,1)) if len(img.shape) < 3 else img\n",
    "        \n",
    "        if img.shape[2] < 3:\n",
    "            print('dimension insufficient')\n",
    "            img = img.reshape((224*224,\n",
    "                               img.shape[2])).T.reshape((img.shape[2],\n",
    "                                                                 224*224))\n",
    "            for i in range(img.shape[0], 3):\n",
    "                img = np.vstack([img, img[0,:]])\n",
    "            img = img.reshape((3,224*224)).T.reshape((224,224,3))\n",
    "        img = img.astype(np.float32)\n",
    "        img = img[:,:,::-1]\n",
    "        # RGB => BGR\n",
    "        for i in range(3):\n",
    "            img[:,:,i] -= MEAN[i]\n",
    "        return img.reshape((224,224,3))\n",
    "\n",
    "    display(Image(img_path))\n",
    "    img = imageio.imread(img_path)\n",
    "    \n",
    "    # load pretrained cnn model\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        img_feature = np.dot(\n",
    "            cnn_mdl.get_output(sess, [process_image(img)])[0].reshape((-1)), U)\n",
    "    img = np.expand_dims(img_feature, axis=0)    \n",
    "    # reset graph for image caption model\n",
    "    tf.reset_default_graph()  \n",
    "    model = ImageCaptionModel(hparams,\"inference\")\n",
    "    model.build()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, 'model/model_90.ckpt') \n",
    "        caption = model.predict(sess, img, dec_map)\n",
    "        print(' '.join(caption))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_90.ckpt\n",
      "a   m a n   i s   f l y i n g   a   k i t e   i n   t h e   a i r\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # reset graph for cnn model\n",
    "U = cPickle.load(open('dataset/U.pkl', 'rb'))  # PCA transforming matrix\n",
    "vgg = PretrainedCNN('pretrained/vgg16_mat.pkl')\n",
    "demo('demo/example3.jpg', vgg, U, dec_map, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到generate出來的caption相當符合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(model, dec_map, img_test, max_len=15):\n",
    "    img_ids, caps = [], []\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    ckpt = tf.train.get_checkpoint_state('model/')\n",
    "    ckpt_dir = 'model/'\n",
    "    ckpt_path = ckpt_dir\n",
    "    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint(ckpt_path))'model/model_%d.ckpt'\n",
    "    saver.restore(sess, 'model/model_100.ckpt')\n",
    "    for img_id, img in img_test.items():\n",
    "        img_ids.append(img_id)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        caps.append(model.predict(sess, img, dec_map))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "              'img_id': img_ids,\n",
    "              'caption': caps\n",
    "            }).set_index(['img_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model_100.ckpt\n"
     ]
    }
   ],
   "source": [
    "# load test image  size=20548\n",
    "img_test = cPickle.load(open('dataset/test_img256.pkl', 'rb'))\n",
    "\n",
    "# create model\n",
    "tf.reset_default_graph()\n",
    "model = ImageCaptionModel(hparams,'inference')\n",
    "model.build()\n",
    "\n",
    "# generate caption to csv file\n",
    "df_predict = generate_captions(model, dec_map, img_test)\n",
    "df_predict.to_csv('generated/demo.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cd CIDErD_win && gen_score -i ../generated/demo.csv -r ../generated/score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們在最後產生score.csv時出現了問題，只有一台電腦可以產生結果，我們到最後還是沒有解決這個問題。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "今天聽完前三名的分享後，首先，發現大家除了試過助教給的CNN外，都有再用更厲害的CNN model 去抽出image 的 representation，representation都比助教給的256維還要大很多，rnn_unit也調成2048，使model的效果進步很多；第二，他們也使用了multilayer的rnn cell或bidirectional RNN去implement，且在每一層都加上了batch norm，使效果更好；第三，他們都有按照助教給的hints，加上Attention或是在Inference時使用beam search。聽完這次分享，前三名的組別都注意到了很多能使效果更好的細節，並且花了許多心力在找尋使效果更好的方法，希望我們下次能夠吸取他們的經驗，拿到更好的表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
