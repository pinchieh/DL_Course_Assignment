{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Member:\n",
    "\n",
    "\n",
    "107062514 賴鵬仁\n",
    "\n",
    "107062616 傅品捷\n",
    "\n",
    "107065513 姚定嘉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing: Cut Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "NUM_PROGRAM = 8\n",
    "programs = []\n",
    "\n",
    "\n",
    "for i in range(1, NUM_PROGRAM+1):\n",
    "    program = pd.read_csv('Program0%d.csv' % (i))\n",
    "    \n",
    "    print('Program %d' % (i))\n",
    "    print('Episodes: %d' % (len(program)))\n",
    "    print(program.columns)\n",
    "    print()\n",
    "    \n",
    "    print(program.loc[:1]['Content'])\n",
    "    print()\n",
    "    \n",
    "    programs.append(program)\n",
    "\n",
    "\n",
    "questions = pd.read_csv('Question.csv')\n",
    "\n",
    "print('Question')\n",
    "print('Episodes: %d' % (len(questions)))\n",
    "print(questions.columns)\n",
    "print()\n",
    "\n",
    "print(questions.loc[:2]['Question'])\n",
    "print()\n",
    "\n",
    "for i in range(6):\n",
    "    print(questions.loc[:2]['Option%d' % (i)])\n",
    "    print()\n",
    "\n",
    "    \n",
    "import jieba\n",
    "\n",
    "jieba.set_dictionary('big5_dict.txt')\n",
    "example_str = '我討厭吃蘋果'\n",
    "cut_example_str = jieba.lcut(example_str)\n",
    "print(cut_example_str)\n",
    "\n",
    "def jieba_lines(lines):\n",
    "    cut_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        cut_line = jieba.lcut(line)\n",
    "        cut_lines.append(cut_line)\n",
    "    \n",
    "    return cut_lines\n",
    "\n",
    "cut_programs = []\n",
    "\n",
    "for program in programs:\n",
    "    episodes = len(program)\n",
    "    cut_program = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        lines = program.loc[episode]['Content'].split('\\n')\n",
    "        cut_program.append(jieba_lines(lines))\n",
    "    \n",
    "    cut_programs.append(cut_program)\n",
    "    \n",
    "print(\"%d programs\" % len(cut_programs))\n",
    "print(\"%d episodes in program 0\" % len(cut_programs[0]))\n",
    "print(\"%d lines in first episode of program 0\" % len(cut_programs[0][0]))\n",
    "\n",
    "print()\n",
    "print(\"first 3 lines in 1st episode of program 0: \")\n",
    "print(cut_programs[0][0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照notebook的做法，用 jieba 將每一句切成以詞為單位的 list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_questions = []\n",
    "n = len(questions)\n",
    "\n",
    "for i in range(n):\n",
    "    cut_question = []\n",
    "    lines = questions.loc[i]['Question'].split('\\n')\n",
    "    cut_question.append(jieba_lines(lines))\n",
    "    \n",
    "    for j in range(6):\n",
    "        line = questions.loc[i]['Option%d' % (j)]\n",
    "        cut_question.append(jieba.lcut(line))\n",
    "    \n",
    "    cut_questions.append(cut_question)\n",
    "print(\"%d questions\" % len(cut_questions))\n",
    "print(len(cut_questions[0]))\n",
    "\n",
    "# 1 question\n",
    "print(cut_questions[0][0])\n",
    "\n",
    "# 6 optional reponses\n",
    "for i in range(1, 7):\n",
    "    print(cut_questions[0][i])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.save('cut_Programs.npy', cut_programs)\n",
    "np.save('cut_Questions.npy', cut_questions)\n",
    "\n",
    "cut_programs = np.load('cut_Programs.npy')\n",
    "cut_Question = np.load('cut_Questions.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將其分別存成 cut_Programs.npy 跟 cut_Questions.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Dictionary & Out-of-Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = dict()\n",
    "def add_word_dict(w):\n",
    "    if not w in word_dict:\n",
    "        word_dict[w] = 1\n",
    "    else:\n",
    "        word_dict[w] += 1\n",
    "for program in cut_programs:\n",
    "    for lines in program:\n",
    "        for line in lines:\n",
    "            for w in line:\n",
    "                add_word_dict(w)\n",
    "for question in cut_questions:\n",
    "    lines = question[0]\n",
    "    for line in lines:\n",
    "        for w in line:\n",
    "            add_word_dict(w)\n",
    "    \n",
    "    for i in range(1, 7):\n",
    "        line = question[i]\n",
    "        for w in line:\n",
    "            add_word_dict(w)\n",
    "import operator\n",
    "\n",
    "word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Total %d words in word_dict\" % len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_SIZE = 75000\n",
    "VOC_START = 1\n",
    "\n",
    "voc_dict = word_dict[VOC_START:VOC_START+VOC_SIZE]\n",
    "print(voc_dict[:10])\n",
    "print()\n",
    "print(\"Total %d words in voc_dict\" % len(voc_dict))\n",
    "np.save('voc_dict.npy', voc_dict)\n",
    "\n",
    "voc_dict = np.load('voc_dict.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_programs = np.load('cut_Programs.npy')\n",
    "cut_questions = np.load('cut_Questions.npy')\n",
    "\n",
    "program_out_a = open('all_refined.txt', 'w', encoding=\"utf-8\") \n",
    "program_out_p = open('programs_refined.txt', 'w', encoding=\"utf-8\")\n",
    "program_out_q = open('questions_refined.txt', 'w', encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "stopword_set = set()\n",
    "stopword_set.add('\\u3000')\n",
    "stopword_set.add('\\t')\n",
    "stopword_set.add(' ')\n",
    "stopword_set.add('...')\n",
    "stopword_set.add('「')\n",
    "stopword_set.add('」')\n",
    "\n",
    "\n",
    "cut_programs_refined = []\n",
    "for program in cut_programs:\n",
    "    episode_refined = []\n",
    "    for episode in program:\n",
    "        line_refined = []\n",
    "        for line in episode:\n",
    "            ex_line = ''\n",
    "            for w in line:\n",
    "                if (w in voc_dict) and (w not in stopword_set):\n",
    "                    ex_line += w + ' '\n",
    "                    line_refined.append(w)\n",
    "            program_out_a.write(ex_line+'\\n')\n",
    "            program_out_p.write(ex_line+'\\n')\n",
    "            \n",
    "    episode_refined.append(line_refined)\n",
    "    \n",
    "cut_programs_refined.append(episode_refined)\n",
    "\n",
    "\n",
    "for question in cut_questions:\n",
    "    for i, line in enumerate(question):\n",
    "        if (i == 0):\n",
    "            ex_line = ''\n",
    "            for qline in question[i]:\n",
    "                for w in qline:\n",
    "                    if (w in voc_dict) and (w not in stopword_set):\n",
    "                        ex_line += w + ' '\n",
    "            program_out_a.write(ex_line+'\\n')\n",
    "            program_out_q.write(ex_line+'\\n')\n",
    "        else:\n",
    "            ex_line = ''\n",
    "            for w in line:\n",
    "                if (w in voc_dict) and (w not in stopword_set):\n",
    "                    ex_line += w + ' '\n",
    "            program_out_a.write(ex_line+'\\n')\n",
    "            program_out_q.write(ex_line+'\\n')\n",
    "            \n",
    "program_out_a.close()\n",
    "program_out_p.close()\n",
    "program_out_q.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many words after cutting, but not all of them are useful. The words too common or too rare can not give us information but may introduce noise. We count the the number of occurrence for each word and remove useless one.\n",
    "\n",
    "此外，我們在分析句子的時候發現，有些句子中有包含 \"\\t\"、\"whitespace\"......等，所以我們將這些不重要的字元過濾掉，並將處理完的資料存起來以利後續使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "sentences = word2vec.Text8Corpus('all_refined.txt')\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, hs=1, negative=0.0001, min_count=1,size = 600)\n",
    "\n",
    "model.save('word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來我們就用 word2vec 去 generate 每一個詞的向量。在這邊我們tune了很多參數，包括(windows, size, sg, hs, negative, min_count...等)。我們在一開始使用size=100，發現效果不佳，經過一番測試，發現size=600的表現最好。\n",
    "windows的部分我們也發現用它default的參數會得到實驗中最好的效果，因此我們windows的部分並沒有調整。而我們min_count設為1的目的是希望可以考慮到只出現過一次的詞，我們認為這些詞在這段文字中也相當重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在train完word2vec model之後也會把model存起來以利後續使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate Training Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "def read_txt(filename):\n",
    "    result = []\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as inFile:\n",
    "        for line in inFile:\n",
    "            result.append(line.strip('\\n'))\n",
    "    return result\n",
    "        \n",
    "programs_refined = read_txt('programs_refined.txt')\n",
    "questions_refined = read_txt('questions_refined.txt')\n",
    "all_refined = read_txt('all_refined.txt')\n",
    "\n",
    "word2vec_model = word2vec.Word2Vec.load('word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_TRAIN = 500000\n",
    "TRAIN_VALID_RATE = 0.7\n",
    "def generate_training_data():\n",
    "    Xs, Ys = [], []\n",
    "    \n",
    "    for i in range(NUM_TRAIN):\n",
    "        pos_or_neg = random.randint(0, 1)\n",
    "        \n",
    "        if pos_or_neg==1:\n",
    "            line_id = random.randint(1, len(programs_refined)-1)\n",
    "            \n",
    "            L1 = programs_refined[line_id].split(' ')[:-1]\n",
    "            L2 = programs_refined[line_id+1].split(' ')[:-1]\n",
    "            L3 = programs_refined[line_id-1].split(' ')[:-1]\n",
    "            \n",
    "            if (L1 and L2 and L3) :\n",
    "                sim1 = word2vec_model.wv.n_similarity(L1, L2)\n",
    "                sim2 = word2vec_model.wv.n_similarity(L1, L3)\n",
    "\n",
    "                if (sim1 > sim2):\n",
    "                    Xs.append(programs_refined[line_id])\n",
    "                    Xs.append(programs_refined[line_id+1])\n",
    "                else:\n",
    "                    Xs.append(programs_refined[line_id-1])\n",
    "                    Xs.append(programs_refined[line_id])\n",
    "\n",
    "                Ys.append(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            first_line_id = random.randint(0, len(programs_refined)-1)\n",
    "            second_line_id = random.randint(0, len(programs_refined)-1)\n",
    "            \n",
    "            L1 = programs_refined[first_line_id].split(' ')[:-1]\n",
    "            L2 = programs_refined[second_line_id].split(' ')[:-1]\n",
    "            \n",
    "            if L1 and L2:\n",
    "                Xs.append(programs_refined[first_line_id])\n",
    "                Xs.append(programs_refined[second_line_id])\n",
    "                Ys.append(0)\n",
    "                    \n",
    "    return Xs, Ys\n",
    "\n",
    "X, Y = generate_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們將train好的word2vec model 載入，將之前做好的資料也一起載入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而我們 generate training data的方法是，我們會 random generate 出postive training data 以及 negative training data。\n",
    "Positive 的部分，我們隨機選一句並去觀察他的上一句與下一句，因為隨機挑出的句子可能會是該episode或是該串對話串的最後一句或是第一句。因此，我們分別比較該句子與其上下的 cosine similarity，取其cosine similarity 高的兩句作為 training data set 的Ｘ，並令其對應的Ｙ為 1。\n",
    "Negative 的部分，我們則就整個 programs 隨機選兩句，作為 training data set 的 Ｘ，並令其對應的Ｙ為 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions_refined:\n",
    "    X.append(question)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(3,4))\n",
    "tfidf.fit(X)\n",
    "#doc_tf = tfidf.transform(X).toarray()\n",
    "doc_tf_sparse = tfidf.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF使用ngram_range = (3, 4)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc_vec = []\n",
    "\n",
    "i = 0\n",
    "for sentence, tf_idf in zip(X, doc_tf_sparse):\n",
    "    words = sentence.split(' ')[:-1]\n",
    "    vec = np.zeros(shape=(600,), dtype=float)\n",
    "    \n",
    "    for w in words:\n",
    "        try:\n",
    "            vec = np.add(vec, word2vec_model.wv[w]*tf_idf[i, tfidf.vocabulary_[w]]*100)\n",
    "        except KeyError:\n",
    "            vec = np.add(vec, word2vec_model.wv[w]*0.1)\n",
    "    \n",
    "    i += 1\n",
    "    doc_vec.append(vec)\n",
    "\n",
    "q_start = len(X) - len(questions_refined)\n",
    "\n",
    "programs_vec = doc_vec[:q_start]\n",
    "questions_vec = doc_vec[q_start:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在取完 TF-IDF 後，我們將每個句子做成向量。我們的方法是，將每一句裡面包含的詞從 word2vec model 中取出該詞的向量，並以他對應的 TF-IDF 值當作weight，最後 sum 起來得到該句子的最終向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = []\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return (np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))) * np.array([1])\n",
    "\n",
    "for _id in range(int(len(programs_vec) / 2)):\n",
    "    X_.append(cosine_sim(programs_vec[_id*2], programs_vec[_id*2+1]))\n",
    "\n",
    "X_ = np.stack(X_, axis=0)\n",
    "y = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後，我們將 training data 中上下兩句的cosine similarity當成餵進model的input。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sklearn Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_, y, test_size=0.3, random_state=0)\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_valid_std = sc.transform(X_valid)\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "las = Lasso(alpha=0.002)\n",
    "\n",
    "las.fit(X_train_std, y_train)\n",
    "y_pred_las = las.predict(X_valid_std)\n",
    "\n",
    "print (\"MSE: %.4f\" % (mean_squared_error(y_valid, y_pred_las)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE: 0.1729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡使用Lasso，因為問題是六選一的選擇題，我們希望model輸出的是每個選項的分數，並挑選分數最高的選項當成prediction的結果；又，我們期望model有較好的generalizability，所以選擇Lasso。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過實驗後，我們發現比起 learning model，feature engineering 相對重要許多。一開始我們的使用 feature hashing 得到的 accuracy 僅僅只有 0.16左右，但再加上 word2vec 與 TF-IDF 後 accuracy 就達到了0.62，接著在 tune 完 word2vec 以及 TF-IDF 的參數後，accuracy 可以達到 0.66。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generate Testing set Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "q_test = []\n",
    "for q_id in range(500):\n",
    "    question = questions_vec[q_id*7]\n",
    "    answer = 0\n",
    "    sim_max = sys.float_info.min\n",
    "    for i in range(1,7):\n",
    "        ans = questions_vec[q_id*7 + i]\n",
    "        set_ = cosine_sim(question, ans)\n",
    "        try:\n",
    "            result = las.predict(set_.reshape(1,-1))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if result > sim_max:\n",
    "            answer = i-1\n",
    "            sim_max = result\n",
    "        \n",
    "    q_test.append((q_id, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data=q_test, columns=['Id', 'Answer']).to_csv('Answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聽完前三名的組別分享，其中兩組都使用了BiLSTM，扣除掉用NN model 外 ，大家的共通點都是有利用更好的切字library 及 比word2vec 更好的fasttext model。在training data 方面，雖然我們有做額外的處理，但都和前幾組相比，做的不夠細膩也考慮的比較不周全。第一名的組別沒用到NN 就可以達到很高的準確率，可見feature engineering 和 train data的生成都是非常的關鍵。 這次比賽過程與最後別人的分享都學到了許多，也期待之後比賽的表現能更進步。謝謝老師與助教!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
